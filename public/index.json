[
{
	"uri": "//localhost:1313/",
	"title": "Data Engineering Immersion Day",
	"tags": [],
	"description": "",
	"content": "Data Engineering Immersion Day Data Engineering Immersion Day là gì Đây là một bài hands-on labs có chứa các module tập trung vào việc sinh, nhập, load, khám phá và sử dụng dữ liệu trong data lake trên AWS\nLợi ích Data Engineering Immersion Day Data Engineering Immersion day cung cấp thời gian hands-on trên các dịch vụ phân tích dữ liệu như Amazon Kinesis Services để truyển tải và phân tích dữ liệu, AWS Data Migration service, AWS Glue, Amazon Athena, Amazon Quicksight. Data Engineering Immersion day giúp xây dựng serverless data lake trên nền tảng đám mây.\nContent Introduction Preparation Connect to EC2 instance Manage session logs Port Forwarding Clean up resources "
},
{
	"uri": "//localhost:1313/3-ingestionwithdms/3.1-dms-migration-lab/",
	"title": " Option1: DMS Migration Lab",
	"tags": [],
	"description": "",
	"content": "DMS Migration Lab Steps Giới thiệu Tạo Subnet Group Tạo Replication Instance Tạo DMS Source Endpoint Tạo Target Endpoint Tạo 1 task để bắt đầu full copy Tạo CDC endpoint để replicate thay đổi Tạo task để replicate liên tục Giới thiệu Bài lab này sẽ giúp bạn hiểu rõ hơn về AWS Database Migration Service(AWS DMS). Bạn sẽ di chuyển dữ liệu từ cơ sở dữ liệu Amazon Relational Database Service (Amazon RDS) Postgres hiện có sang Amazon Simple Storage Service (Amazon S3) bucket.\nLink GitHub của lab - https://github.com/aws-samples/data-engineering-for-aws-immersion-day\nTạo Subnet group Tại DMS console , chọn Subnet Groups và Create subnet group. Tại Name textbox: dms-lab-subnet-grp Description textbox: Replication instance for production data system VPC: Chọn *-dmslstudv1 Chọn subnets và click Add Chọn Create subnet group\nTại DMS console, subnet group displays Complete Tạo Replication Instance Tại DMS console, chọn Replication instances để tạo replication instance mới. Name: DMS-Replication-Instance\nDescription: DMS Replication Instance\nInstance class: dms.t3.medium\nChọn engine version mới nhất\nHigh Availability: Dev or test workload (Single-AZ)\nVPC: dmslstudv1 Chọn Advanced để mở rộng\nChọn security group là sgdefault Tất cả các trường còn lại mặc định\nDMS console hiển thị trạng thái tạo instance Tạo MDS Source Endpoint Tại DMS console, chọn Endpoints để tạo source Endpoint Chọn Source Endpoint\nEndpoint identifier: rds-source-endpoint\nSource engine: PostgreSQL\nAccess to Endpoint database: Provide access information manually\nServer name: RDS-Server-Name Port: 5432\nSSL mode: none\nUser name: adminuser\nPassword: admin123\nDatabase name: sportstickets\nTất cả còn lại để mặc định, rồi click tạo endpoint. Khi sẵn sàng, trạng thái sẽ chuyển sang active Kiểm tra lại replication instance Chọn source endpoint và nhấn Test connection Click Run test. Nếu thành công sẽ có thông báo \u0026ldquo;Connection tested successfully\u0026rdquo; xuất hiện. Tạo Target Endpoint Tại DMS console, Chọn Endpoint để tạo target Endpoint Endpoint type: Target endpoint Endpoint identifier: s3-target-endpoint Target engine: Amazon S3 Service access role ARN: Copy và Past DMSLabRoleS3 ARN Bucket name: paste S3 Bucket Name Bucket folder: tickets Mở rộng phần: Endpoint settings\nChọn Use endpoint connection checkbox, điền addColumnName=true trong Extra connection attributes box Mở rộng Test endpoint connection (optional). chọn VPC.\nChọn Run test. Bước này kết nối với source database. Nếu thành công sẽ hiển thị thông báo \u0026ldquo;Connection tested successfully\u0026rdquo; Chọn Create Endpoint. Khi đã sẵn sàng, trạng thái endpoint sẽ chuyển thành active Tạo task initial full copy Tại DMS console, chọn Database Migration Tasks. Chọn Create Task.\nTask name: dms-full-dump-task Chọn Replication instance Chọn Source endpoint Chọn Target endpoint Migration type: Migrate existing data. Mở rộng Task Settings Chọn Turn on CloudWatch logs checkbox Tại Table Mappings Chọn Add new selection rule và chọn Enter a Schema tại Schema field Tại Source name: dms_sample Để tất cả field còn lại mặc định. Chọn Create task. Task sẽ được tạo và tự động start\nChọn task và xem chi tiết. Khi hoàn thành, task console hiển thị 100% progress Mở S3 console và xem data được copy bởi DMS Review data bằng S3 select\nTạo CDC endpoint để replicate các thay đổi diễn ra Tại DMS console, chọn Endpoints Nhấn Create endpoint\nEndpoint type: Target\nEndpoint identifier: rds-cdc-endpoint\nTarget engine: Amazon S3\nService Access Role ARN: DMSLabRoleS3\nBucket name: Chọn S3 Bucketname\nBucket folder: cdc Mở rộng phần Endpoint settings\nTích vào checkbox Use endpoint connection attributes và nhập addColumnName=true. Thuộc tính này bao gồm tên cột từ dữ liệu nguồn. Mở rộng phần Test endpoint connection (optional), chọn VPC name.\nClick Run test. Nếu thành công sẽ hiển thị thông báo \u0026ldquo;Connection tested successfully\u0026rdquo;.\nChọn Create endpoint Khi sẵn sàng, endpoint status chuyển sang active.\nTạo task replication liên tục. Tại DMS console, chọn Database Migration Tasks Chọn Create Task Task Identifier: cdctask Chọn Replication instance Chọn Source endpoint Chọn Target endpoint: rds-cdc-endpoint Chọn Migration type: Replicate data changes only. Trong Task Settings, Chọn Turn on CloudWatch logs checkbox Chuyển đến Table Mappings\nChọn ** Add new selection rule ** và Chọn Enter a Schema tại Schema field\nTại Source name, chọn dms_sample. Tất cả còn lại để mặc định Chọn Create task. Task sẽ được tạo và tự động chạy. Chúng ta có thể thấy trạng thái là Replication ongoing. Đợi 5 đến 10 phút để CDC data ánh xạ RDS postgres database\nChọn CDC task để xem chi tiết, xem phần Table statistics: Mở S3 console và xem CDC data được copied từ DMS\nChọn 1 file và sử dụng S3 Select "
},
{
	"uri": "//localhost:1313/5-labqueryandvisualize/5.1-athenaandquicksight/",
	"title": "Athena and QuickSight",
	"tags": [],
	"description": "",
	"content": "Step Yêu cầu Bắt đầu Truy vấn data với Amazon Athena Dựng Amazon QuickSight Dashboard Set up QuickSight Tạo QuickSight Charts Tạo QuickSight Parameters Tạo QuickSight Filter Thêm Calculated Fields Amazon QuickSight ML-Insights (Optional) Athena Workgroups to Control Query Access and Costs (Optional) Workflow setup to separate workloads Explore the features of workgroups Managing Query Usage and Cost Sử dụng tag để phân bổ chi phí Yêu cầu Hoàn thành Ingestion with DMS và Transforming data with Glue ETL labs.\nBắt đẩu Trong bài lab này, chúng ta sẽ hoàn thành các task sau:\nTruy vấn dữ liệu và tạo view với Amazon Athena Athena Workgroups để Điều khiển query access và costs Dựng dashboard với Amazon QuickSight Truy vấn dữ liệu với Amazon Athena Truy cập Athena Chọn database \u0026ldquo;ticketdata\u0026rdquo; Chọn table \u0026ldquo;parquet_sporting_event_ticket\u0026rdquo; để xem các trường\nCopy đoạn SQL sau và nhấn Run\nSELECT\re.id AS event_id,\re.sport_type_name AS sport,\re.start_date_time AS event_date_time,\rh.name AS home_team,\ra.name AS away_team,\rl.name AS location,\rl.city\rFROM parquet_sporting_event e,\rparquet_sport_team h,\rparquet_sport_team a,\rparquet_sport_location l\rWHERE\re.home_team_id = h.id\rAND e.away_team_id = a.id\rAND e.location_id = l.id; Kết quả như sau:\nChọn Create và View from query\nĐặt tên view là sporting_event_info vào click Create View đã được khởi tạo Copy đoạn Sql sau sang một tab mới SELECT t.id AS ticket_id,\re.event_id,\re.sport,\re.event_date_time,\re.home_team,\re.away_team,\re.location,\re.city,\rt.seat_level,\rt.seat_section,\rt.seat_row,\rt.seat,\rt.ticket_price,\rp.full_name AS ticketholder\rFROM sporting_event_info e,\rparquet_sporting_event_ticket t,\rparquet_person p\rWHERE\rt.sporting_event_id = e.event_id\rAND t.ticketholder_id = p.id Lưu lại với tên: create_view_sporting_event_ticket_info Click Run\nKết quả xuất hiện như sau: Đặt tên view là: sporting_event_ticket_info và click Create Copy đoạn Sql sau sang một tab mới\nSELECT\rsport,\rcount(distinct location) as locations,\rcount(distinct event_id) as events,\rcount(*) as tickets,\ravg(ticket_price) as avg_ticket_price\rFROM sporting_event_ticket_info\rGROUP BY 1\rORDER BY 1; Lưu với tên là: analytics_sporting_event_ticket_info Click Run\nBuild an Amazon QuickSight Dashboard Set up QuickSight Truy cập QuickSight Sign up for QuickSight.\nChọn Enterprise Version\nContinue Chọn No, Maybe Later, Ở màn hình mới để mặc định.\nChọn region và check vào các box, enable auto discovery, Amazon Athena, and Amazon S3. Nhập tên và email cho account QuickSight\nChọn DMS bucket và chọn Finish\nNhấn Finish\nChọn New analysis Chọn New dataset Tại màn Create a Dataset, chọn Athena\nData source name: ticketdata-qs, và chọn Validate connection\nClick Create data source Chọn database ticketdata\nChọn bảng sporting_event_ticket_info và nhấn Select\nChọn option Import to SPICE for quicker analytics và nhấn Visualize. Chúng ta có thể quan sát thấy QuickSight Visualize interface và bắt đầu xây dựng dashboard\nTạo QuickSight Charts Trong phần này chúng ta sẽ cùng tìm hiểu các loại chart\nChọn cột ticket_price để tạo chart.\nChọn expand icon và chọn Show as Currency Click Add button từ Visuals pane\nVisual types: Vertical bar chart X-axis Fields list chọn event_date_time Y-axis chọn ticket_price từ Field list. Bạn có thể kéo và di chuyển các hình ảnh khác để điều chỉnh không gian trong trang tổng quan. Trong danh sách Trường, nhấp và kéo trường Seat_level vào hộp Nhóm/Màu. Bạn cũng có thể sử dụng thanh trượt bên dưới trục x để vừa với tất cả dữ liệu. Trong Visuals pane, chọn Vertical bar chart và Clustered bar combo chart icon\nTại Fields list, chọn ticketholder\nTại Lines box, chọn Aggregate: Count Distinct.\nChọn insight icon Tạo QuickSight Parameters Trong phần này chúng ta sẽ tạo parameters cho dashboard và thêm filter.\nChọn Parameters từ tool bar Chọn Add để tạo parameter\nĐặt tên là EventFrom\nChọn Data type là Datetime\nTime granularity là Hour\nHãy chọn giá trị từ lịch làm ngày bắt đầu có sẵn trong biểu đồ của bạn cho event_date_time\nChọn Create và đóng dialog Tạo thêm parameter với thông số sau:\nName: EventTo Chọn Data type là Datetime Time granularity là Hour Hãy chọn giá trị từ lịch làm ngày bắt đầu có sẵn trong biểu đồ của bạn cho event_date_time Click Create Trong màn hình tiếp theo. Click vào 3 chấm của EventFrom parameter và chọn Add control Nhập name là Event From và nhấn Add Tương tự với EventTo Bây giờ bạn có thể xem và mở rộng phần Kiểm soát phía trên biểu đồ.\nTạo QuickSight Filter Chọn Filter Click (+) để thêm filter event_date_time Chọn edit Chọn All applicable visuals và Applied To dropdown\nFilter type chọn Date \u0026amp; Time range và Condition là Between\nChọn option Use Parameter, chọn Yes\nStart date parameter: EventFrom\nEnd date parameter: EventTo Click Apply.\nThêm Calculated Fields Chọn Add Calculated Field Nhập name là: event_day_of_week\nFormula: extract(\u0026lsquo;WD\u0026rsquo;,{event_date_time})\nClick Save Thêm một trường được tính toán khác với các thuộc tính sau:\nCalculated field name: event_hour_of_day Formula: extract(\u0026lsquo;HH\u0026rsquo;,{event_date_time}) Chọn Visualize icon từ the Tool bar và nhấn Add visual Field type\u0026quot; scatter plot\nTại Fields list, Chọn các thuộc tính sau:\nX-axis: \u0026ldquo;event_hour_of_day\u0026rdquo; Y-axis: \u0026ldquo;event_day_of_week\u0026rdquo; Size: \u0026ldquo;ticket_price\u0026rdquo; Pushlish hoặc Share Dashboard là ảnh chụp nhanh phân tích chỉ đọc mà bạn có thể chia sẻ với những người dùng Amazon QuickSight khác nhằm mục đích báo cáo. Trong Dashboard, những người dùng khác vẫn có thể xem hình ảnh và dữ liệu nhưng điều đó sẽ không sửa đổi tập dữ liệu.\nBạn có thể chia sẻ phân tích với một hoặc nhiều người dùng khác mà bạn muốn cộng tác để tạo hình ảnh. Phân tích cung cấp các cách sử dụng khác để ghi và sửa đổi tập dữ liệu.\nCost Allocation Tags Khi bạn tạo hai nhóm làm việc: nhóm làm việcA và nhóm làm việcB, bạn cũng đã tạo tên dưới dạng thẻ. Các thẻ này có thể được sử dụng trong bảng điều khiển Quản lý chi phí và thanh toán để xác định mức sử dụng cho mỗi nhóm làm việc.\nVí dụ: bạn có thể tạo một bộ thẻ cho nhóm làm việc trong tài khoản của mình để giúp bạn theo dõi chủ sở hữu nhóm làm việc hoặc xác định nhóm làm việc theo mục đích của họ. Bạn có thể xem các thẻ cho một nhóm làm việc trong trang “Xem chi tiết” cho nhóm làm việc đang được xem xét.\nBạn có thể thêm tag sau khi đã tạo workgroup. Để tạo tag:\nAthena console, chọn tab Workgroups và chọn workgroup. Chọn tab tags. Trên tab tags, bấm vào Manage tags, sau đó chỉ định key và value cho tag. Khi bạn hoàn tất, hãy nhấp vào Save Changes. "
},
{
	"uri": "//localhost:1313/4-transformingdatawithglue/4.1-datavalidationandetl/",
	"title": "Data Validation and ETL",
	"tags": [],
	"description": "",
	"content": "PART A: Tạo Glue Crawler để load full data Truy cập AWS Glue Console Tại AWS Glue menu, chọn Crawlers Chọn Create crawler.\nNhập glue-lab-crawler làm crawler name để init load\nCó thể nhập hoặc không description và nhấn next. Chọn Not yet và Add a data source\nChọn S3 làm data source, S3 Path là path chứa CSV files từ bài lab DMS, tất cả các tham số còn lại để default và nhấn Add an S3 data source\nChọn Next Chọn Iam Role\nChọn Next Chọn Add database\nNhập ticketdata là tên database và nhấn Create database\nChọn Target database là ticketdata vừa tạo và nhấn next Review và nhấn Create crawler. Thực hiện Crawler bằng cách nhấn Run crawler Tại AWS Glue chọn Databases -\u0026gt; Tables\nPART B: Xác thực dữ liệu Chọn ticketdata database, person tables Tại table này sẽ có một số cột không thể xác định tên. Chúng ta sẽ khắc phục nó. Chọn Edit Schema Chọn colr0 và nhấn Edit Nhập id làm column name và nhấn Save Lặp lại các bước trên với từng các cột còn lại: full_name, last_name and first_name. Nhấn Save as new table version. PART C: Data ETL Chọn ETL jobs. Chọn Visual ETL Chọn Amazon S3 từ Sources list để thêm Data source - S3 bucket Quan sát data source properties. Chọn ticketdata database, chọn tables sport_team Chọn Change Schema để thêm Transform - Change Schema node.\nQuan sát properties của Transform - Change Schema node. Đổi type của id thành double Chọn S3 là target. Chọn Data target - S3 bucket để xem thuộc tính. Đổi Format thành Parquet. Tại Compression Type chọn Uncompressed\nChọn S3 Target Location, nhấn Browse S3 và chọn tickets item trong \u0026ldquo;dmslabs3bucket\u0026rdquo; bucket và nhấn Choose\nThêm dms_parquet/sport_team/ vào S3 url. Chọn Job details và Nhập tên là Glue-Lab-SportTeamParquet.\nChọn IAM Role\nTại Job bookmark, chọn Disable. Chúng ta sẽ thực hành bookmark tại bài lab tiếp theo/ Nhấn Save button để tạo job\nKhi thấy thông báo Successfully created job, chọn Run để bắt đầu job.\nChọn Jobs phía panel bên trái để xem list jobs.\nChọn Monitoring để xem thống kê trạng thái và số lân run. Chọn Job run để xác định ETL job đã chạy thành công. Mất khoảng tầm 1 phút.\nLặp lại các bước trên cho 4 tables sport_location, sporting_event, sporting_event_ticket and person tables PART D: Tạo Glue Crawler cho Parquet Files Tại Glue navigation menu, chọn Create crawler. Nhập glue-lab-parquet-crawler làm Crawler name và nhấn Next Chọn Not yet và Add a data source\nChọn S3 làm data source Chọn Next\nChọn IAM role Chọn ticketdata làm database Review lại và nhấn Create crawler\nChọn Run Crawler Quan sát tables 10. Chọn Tables 11. Chọn filter parquet và quan sát\nChúng ta đã hoàn thành bài lab Data Validation and ETL\n"
},
{
	"uri": "//localhost:1313/7-bonuslabgluedatabrew/7.1-databrewpre-lab/",
	"title": "DataBrew Pre-Lab",
	"tags": [],
	"description": "",
	"content": "For our EC2 instances to be able to send session logs to the S3 bucket, we will need to update the IAM Role assigned to the EC2 instance by adding a policy that allows access to S3.\nUpdate IAM Role Go to IAM service management console Click Roles. In the search box, enter SSM. Click on the SSM-Role role. Click Attach policies. In the Search box enter S3. Click the policy AmazonS3FullAccess. Click Attach policy. In the production environment, we will grant stricter permissions to the specified S3 bucket. In the framework of this lab, we use the policy AmazonS3FullAccess for convenience.\nNext, we will proceed to create an S3 bucket to store session logs.\n"
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Kiến trúc của bài lab Nhập dữ liệu từ RDS(Postgres) vào S3 bằng AWS Database Migration Service Chuyển đổi dữ liệu với các service Glue và lưu lại vào S3 Khám phá dữ liệu bằng các công cụ truy vấn và trực quan hóa dữ liệu. Áp dụng Machine Learning sử dụng dịch vụ SageMaker "
},
{
	"uri": "//localhost:1313/6-labdatalakeautomation/6.1-lakeformationlab/",
	"title": "Lake Formation Lab",
	"tags": [],
	"description": "",
	"content": "Tạo Glue JDBC connection for RDS Truy cập AWS Glue console. Chọn Connections Chọn Create connection. Nhập tên là glue-rds-connection Chọn JDBC connection type Nhập miêu tả Chọn JDBC URL "
},
{
	"uri": "//localhost:1313/2-lab1/2.1-prepare/",
	"title": "Phát hiện luồng click chuột bất thường bằng Amazon Managed Service for Apache Flink",
	"tags": [],
	"description": "",
	"content": "Các bước thực hiện Giới thiệu Deploy CloudFormation Stack Thiết lập Amazon Kinesis Data Generator Thiết lập Email và SMS Subscription Xem xét AWS Lambda Anomaly function Phụ lục: CloudFormation Template Giới thiệu Phần này sẽ là phần thiết lập môi trường cho bài lab.\nSau khi deploy CloudFormation template Kinesis_PreLab.yaml ta sẽ được kiến trúc sau:\nCloudFormation template sẽ tạo các resource sau:\n2 S3 buckets: dùng để lưu trữ dữ liệu thô và dữ liệu đã được xử lý 1 Lambda function: Function này sẽ được trigger khi phát hiện bất thường. Amazon SNS topic: Lamda function sẽ publish tới topic này khi phát hiện click bất thường. Amazon Cognito User credentials: Sử dụng để login vào Kinesis Data Generator để gửi bản ghi đến Kinesis Data Firehose. CloudFormation Stack Deployment Click vào đây để deploy CloudFormation Stack: Deploy To AWS\rNhập các tham số vào trong form:\nĐiền các thông số trong form: Username: Username để đăng nhập vào Kinesis Data Generator Password: Mật khẩu để đăng nhập Kinesis Data Generator. Email: Email để nhận thông báo. SNS topic sẽ gửi mail xác nhận. SMS: Số điện thoại nhận thông báo từ SNS. Ở phần cuối, check vào box marked \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources\u0026rdquo;. Chọn Create. CloudFormation chuyển hướng bạn đến ngăn xếp hiện có của bạn. Sau vài phút, kinesis-pre-lab hiển thị trạng thái CREATE_COMPLETE. Trong khi stack runs, ta sẽ nhân được email như sau: Xác nhập subscription Khi stack deployed, click Outputs để xem thêm thông tin:\nKinesisDataGeneratorUrl: Đây là Kinesis Data Generator (KDG) URL RawBucketName: Tên bucket lưu raw data từ Kinesis Data Generator (KDG) ProcessedBucketName: Tên bucket lưu transformed data Bạn đã hoàn thành việc triển khai CloudFormation.\nKhởi tạo Amazon Kinesis Data Generator (KDG) Tại tab Outputs. Click vào Kinesis Data Generator URL.\nKDG đơn giản hóa nhiệm vụ tạo dữ liệu và gửi dữ liệu tới Amazon Kinesis. Công cụ này cung cấp giao diện người dùng thân thiện với người dùng chạy trực tiếp trong trình duyệt của bạn. Với KDG, bạn có thể thực hiện các tác vụ sau:\nTạo template cho bản ghi trong các trường hợp cụ thể . Tạo dữ liệu cho template với dữ liệu cố định hoặc dữ liệu ngẫu nhiên. Lưu lại template tương lai. Liên tục gửi hàng nghìn bản ghi mỗi giây tới Amazon Kinesis data stream hoặc Firehose delivery stream. Kiểm tra Cognito User trong Kinesis Data Generator.\nClick KinesisDataGeneratorUrl trong Outputs tab Đăng nhập bằng username và password nhập vào CloudFormation console Sau khi đăng nhập, bạn sẽ thấy bảng điều khiển Kinesis Data Generator. Cần thiết lập một số template để giả lập dòng nhấp chuột Tạo các template sau nhưng chưa nhấp vào Gửi dữ liệu, chúng ta sẽ thực hiện việc đó sau. Sao chép phần tô sáng tên tab bằng chữ in đậm và giá trị dưới dạng chuỗi JSON, tham khảo ảnh chụp màn hình:\nSchema Discovery Payload {\u0026#34;browseraction\u0026#34;:\u0026#34;DiscoveryKinesisTest\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;yourwebsiteurl.domain.com\u0026#34;} Click Payload {\u0026#34;browseraction\u0026#34;:\u0026#34;Click\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;yourwebsiteurl.domain.com\u0026#34;} Impression Payload {\u0026#34;browseraction\u0026#34;:\u0026#34;Impression\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;yourwebsiteurl.domain.com\u0026#34;} Amazon Kinesis Data Generator sẽ trông như thế này Xác thực email và SMS Subscription Tại Amazon SNS, chọn Topics để xem. Click vào topic để xem chi tiết: AWS Lambda function Chọn vào Amazon Lamba để xem các lambda function mà CloudFormation đã tạo cho ta: Chọn vào lambda function để xem chi tiết: Tại thời điểm này, chúng ta đã có tất cả các thành phần cần thiết để làm lab.\n"
},
{
	"uri": "//localhost:1313/5-labqueryandvisualize/5.2-athenafederatedquery/",
	"title": "Athena Federated query",
	"tags": [],
	"description": "",
	"content": "Federated query là một tính năng mới của Amazon Athena cho phép các nhà phân tích dữ liệu, kỹ sư và nhà khoa học dữ liệu thực hiện các truy vấn SQL trên dữ liệu được lưu trữ trong các nguồn dữ liệu quan hệ, không quan hệ, đối tượng và tùy chỉnh. Với Athena federated query, khách hàng có thể gửi một truy vấn SQL duy nhất và phân tích dữ liệu từ nhiều nguồn chạy tại chỗ hoặc được lưu trữ trên đám mây. Athena thực hiện federated queries bằng Data Source Connectors chạy trên AWS Lambda.\nTrong bài này, chúng ta sẽ khám phá cách truy vấn các nguồn dữ liệu khác nhau từ Athena. Chúng tôi sẽ sử dụng cơ sở dữ liệu RDS hiện có được tạo như một phần của thiết lập ban đầu dưới dạng datasource-1 và data lake data (được lưu trữ trong S3) dưới dạng datasource-2.\nYêu cầu Hoàn thành Ingestion with DMS và Transforming data with Glue ETL labs.\nStep Tạo S3 endpoint để cho phép S3 truy cập vào Athena connector for Lambda, hãy làm theo các bước được đề cập ở đây để tạo S3 endpoint. Đảm bảo sử dụng cùng subnet sẽ được sử dụng cho hàm Lambda trong bước tiếp theo. Tại Athena console, chọn \u0026ldquo;Data sources\u0026rdquo;, Create data source button Chọn PostgreSQL làm data source, và nhấn Next Nhập Data source name: Postgres_DB\nTại Connection detail, Chọn ‘Create Lambda function’ Điền các thông tin như sau Field Value Application Name AthenaJdbcConnector SecretNamePrefix AthenaFed_ SpillBucket Get the BucketName from Workshop Studio Event Dashboard (e.g. dmslab-student-dmslabs3bucket-dpxexymdkhve) DefaultConnectionString postgres://jdbc:postgresql://\u0026lt;DATABASE_ENDPOINT\u0026gt;:5432/sportstickets?user=adminuser\u0026amp;password=admin123 → replace \u0026lt;DATABASE_EDNPOINT\u0026gt; with the correct database endpoint (e.g. dmslabinstance.abcdshic87yz.eu-west-1.rds.amazonaws.com) DisableSpillEncryption false LambdaFunctionName postgresqlconnector LambdaMemory 3008 LambdaTimeout 900 SecurityGroupIds Use the SecurityGroupId noted in prerequisites SpillPrefix athena-spill SubnetIds Use the SubnetId noted in prerequisites Đợi function deploy, Chọn function ở ô Select or enter a Lambda function. Nhấn Next Nhấn Create data source\nCấu hình biến môi trường cho Lamda function. Thêm một biến môi trường có tên là: Postgres_DB_connection_string và copy dữ liệu của biến default Xác thực data source đã được tạo ở Athena Console. Truy cập query editor chọn new datasource\nTruy vấn kết hợp sử dụng, “sport_location” tables của Postgres data source và “parquet_sporting_event” table từ data lake Copy đoạn code sau và chạy:\nSELECT loc.city, count(distinct evt.id) AS events\rFROM \u0026#34;Postgres_DB\u0026#34;.\u0026#34;dms_sample\u0026#34;.\u0026#34;sport_location\u0026#34; AS loc\rJOIN \u0026#34;AwsDataCatalog\u0026#34;.\u0026#34;ticketdata\u0026#34;.\u0026#34;parquet_sporting_event\u0026#34; AS evt\rON loc.id = evt.location_id\rGROUP BY loc.city\rORDER BY loc.city ASC; "
},
{
	"uri": "//localhost:1313/7-bonuslabgluedatabrew/7.2-datapreparationwithgluedatabrew/",
	"title": "Data preparation with Glue DataBrew",
	"tags": [],
	"description": "",
	"content": "For our EC2 instances to be able to send session logs to the S3 bucket, we will need to update the IAM Role assigned to the EC2 instance by adding a policy that allows access to S3.\nUpdate IAM Role Go to IAM service management console Click Roles. In the search box, enter SSM. Click on the SSM-Role role. Click Attach policies. In the Search box enter S3. Click the policy AmazonS3FullAccess. Click Attach policy. In the production environment, we will grant stricter permissions to the specified S3 bucket. In the framework of this lab, we use the policy AmazonS3FullAccess for convenience.\nNext, we will proceed to create an S3 bucket to store session logs.\n"
},
{
	"uri": "//localhost:1313/4-transformingdatawithglue/4.2-hudi/",
	"title": "Incremental Data Processing with Hudi",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Apache HUDI Apache Hudi là một framework trừu tượng hóa lưu trữ giúp các tổ chức phân tán xây dựng và quản lý các data lakes có quy mô petabyte. Bằng cách sử dụng các phương pháp nguyên thủy như upsert và incremental pulls, Hudi mang đến khả năng xử lý kiểu luồng cho dữ liệu lớn batch-like big data. Hudi cho phép Atomicity, Consistency, Isolation \u0026amp; Durability (ACID) trên data lake.\nDưới đây là một số tài liệu tham khảo:\nApache Hudi concepts How Hudi Works Trong bài lab này, chúng ta sẽ học được:\nHow to create HUDI tables. Process incremental updates. Perform upsert operations. Run incremental queries in a Glue job. Step Step 0 - Các yêu cầu Step 1 - Tạo glue job và HUDI tables Step 2 - Truy vấn HUDI tables bằng Athena Step 3 - HUDI configurations Step 4 - Upsert CDC data. Step 5 - Incremental Queries sử dụng Spark SQL. Step 0 - Prerequisites Hoàn thành Main Lab or Autocomplete DMS Lab\nSource RDS database và trạng thái cdctask là ‘Replication ongoing’ Tên của S3 bucket trong s3-target-endpoint DMS Step 1 - Tạo Glue job và HUDI tables Truy cập AWS Glue Console và chọn Jobs Chọn Spark script editor, nhấn Create a new script with boilerplate code option và click Create Copy đoạn code sau và past vào Glue script editor\nfrom pyspark.sql.types import StringType\rimport sys\rimport os\rimport json\rfrom pyspark.context import SparkContext\rfrom pyspark.sql.session import SparkSession\rfrom pyspark.sql.functions import concat, col, lit, to_timestamp, dense_rank, desc\rfrom pyspark.sql.window import Window\rfrom awsglue.utils import getResolvedOptions\rfrom awsglue.context import GlueContext\rfrom awsglue.job import Job\rfrom awsglue.dynamicframe import DynamicFrame\rimport boto3\rfrom botocore.exceptions import ClientError\rargs = getResolvedOptions(sys.argv, [\u0026#39;JOB_NAME\u0026#39;, \u0026#39;RAWZONE_BUCKET\u0026#39;, \u0026#39;CURATED_BUCKET\u0026#39;])\rspark = SparkSession.builder.config(\u0026#39;spark.serializer\u0026#39;,\u0026#39;org.apache.spark.serializer.KryoSerializer\u0026#39;).config(\u0026#39;spark.sql.hive.convertMetastoreParquet\u0026#39;, \u0026#39;false\u0026#39;).getOrCreate()\rglueContext = GlueContext(spark.sparkContext)\rjob = Job(glueContext)\rjob.init(args[\u0026#39;JOB_NAME\u0026#39;], args)\rlogger = glueContext.get_logger()\rlogger.info(\u0026#39;Initialization.\u0026#39;)\rglueClient = boto3.client(\u0026#39;glue\u0026#39;)\rlogger.info(\u0026#39;Fetching configuration.\u0026#39;)\rregion = os.environ[\u0026#39;AWS_DEFAULT_REGION\u0026#39;]\rrawBucketName = args[\u0026#39;RAWZONE_BUCKET\u0026#39;]\rcuratedBucketName = args[\u0026#39;CURATED_BUCKET\u0026#39;]\rif rawBucketName == None or curatedBucketName == None:\rraise Exception(\u0026#34;Please input the bucket names in job parameters. Refer: https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-get-resolved-options.html\u0026#34;)\rrawS3TablePath = \u0026#39;s3://\u0026#39; + rawBucketName + \u0026#39;/tickets/dms_sample/ticket_purchase_hist/\u0026#39;\rcdcRawS3TablePath = \u0026#39;s3://\u0026#39; + rawBucketName + \u0026#39;/cdc/dms_sample/ticket_purchase_hist/\u0026#39;\rcuratedS3TablePathPrefix = \u0026#39;s3://\u0026#39; + curatedBucketName + \u0026#39;/hudi/\u0026#39;\rsourceDBName = \u0026#39;dms_sample\u0026#39;\rsourceTableName = \u0026#39;ticket_purchase_hist\u0026#39;\rtargetDBName = \u0026#39;hudi_sample\u0026#39;\rtargetTableName = \u0026#39;ticket_purchase_hist\u0026#39;\rhudiStorageType = \u0026#39;CoW\u0026#39;\rdropColumnList = [\u0026#39;db\u0026#39;,\u0026#39;table_name\u0026#39;,\u0026#39;Op\u0026#39;]\rlogger.info(\u0026#39;Processing starts.\u0026#39;)\rkeys = {\r\u0026#34;dms_sample.ticket_purchase_hist\u0026#34;: {\u0026#34;primaryKey\u0026#34;: \u0026#34;sporting_event_ticket_id\u0026#34;}\r}\rspark.sql(\u0026#39;CREATE DATABASE IF NOT EXISTS \u0026#39; + targetDBName)\risTableExists = False\risPrimaryKey = False\risPartitionKey = False\rprimaryKey = \u0026#39;\u0026#39;\rpartitionKey = \u0026#39;\u0026#39;\rtry:\rglueClient.get_table(DatabaseName=targetDBName,Name=targetTableName)\risTableExists = True\rlogger.info(targetDBName + \u0026#39;.\u0026#39; + targetTableName + \u0026#39; exists.\u0026#39;)\rexcept ClientError as e:\rif e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] == \u0026#39;EntityNotFoundException\u0026#39;:\risTableExists = False\rlogger.info(targetDBName + \u0026#39;.\u0026#39; + targetTableName + \u0026#39; does not exist. Table will be created.\u0026#39;)\r# lookup primary key and partition keys from keys json declared above\rtry:\rkeyName = sourceDBName +\u0026#34;.\u0026#34; + sourceTableName\rlogger.info(keyName)\rtable_config = \u0026#39;\u0026#39;\rfor key in keys:\rif key == keyName:\rtable_config = keys[key]\rtry:\rprimaryKey = table_config[\u0026#39;primaryKey\u0026#39;]\risPrimaryKey = True\rlogger.info(\u0026#39;Primary key:\u0026#39; + primaryKey)\rexcept KeyError as e:\risPrimaryKey = False\rlogger.info(\u0026#39;Primary key not found. An append only glueparquet table will be created.\u0026#39;)\rtry:\rpartitionKey = table_config[\u0026#39;partitionKey\u0026#39;]\risPartitionKey = True\rlogger.info(\u0026#39;Partition key:\u0026#39; + partitionKey)\rexcept KeyError as e:\risPartitionKey = False\rlogger.info(\u0026#39;Partition key not found. Partitions will not be created.\u0026#39;)\rexcept ClientError as e: if e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] == \u0026#39;ParameterNotFound\u0026#39;:\risPrimaryKey = False\risPartitionKey = False\rlogger.info(\u0026#39;Config for \u0026#39; + sourceDBName + \u0026#39;.\u0026#39; + sourceTableName + \u0026#39; not found in parameter store. Non partitioned append only table will be created.\u0026#39;)\r# Reads the raw zone table and writes to HUDI table\rtry:\rinputDyf = glueContext.create_dynamic_frame_from_options(connection_type = \u0026#39;s3\u0026#39;, connection_options = {\u0026#39;paths\u0026#39;: [rawS3TablePath], \u0026#39;groupFiles\u0026#39;: \u0026#39;none\u0026#39;, \u0026#39;recurse\u0026#39;:True}, format = \u0026#39;csv\u0026#39;, format_options={\u0026#39;withHeader\u0026#39;:True}, transformation_ctx = targetTableName)\r# Ensure timestamp is in HUDI timestamp format\rinputDf = inputDyf.toDF().withColumn(\u0026#39;transaction_date_time\u0026#39;, to_timestamp(col(\u0026#39;transaction_date_time\u0026#39;))).withColumn(primaryKey, col(primaryKey).cast(StringType()))\rlogger.info(\u0026#39;Total record count in raw table = \u0026#39; + str(inputDyf.count()))\rtargetPath = curatedS3TablePathPrefix + \u0026#39;/\u0026#39; + targetDBName + \u0026#39;/\u0026#39; + targetTableName\rmorConfig = {\r\u0026#39;hoodie.datasource.write.table.type\u0026#39;: \u0026#39;MERGE_ON_READ\u0026#39;, \u0026#39;hoodie.compact.inline\u0026#39;: \u0026#39;false\u0026#39;, \u0026#39;hoodie.compact.inline.max.delta.commits\u0026#39;: 20, \u0026#39;hoodie.parquet.small.file.limit\u0026#39;: 0\r}\rcommonConfig = {\r\u0026#39;className\u0026#39; : \u0026#39;org.apache.hudi\u0026#39;, \u0026#39;hoodie.datasource.hive_sync.use_jdbc\u0026#39;:\u0026#39;false\u0026#39;, \u0026#39;hoodie.datasource.write.precombine.field\u0026#39;: \u0026#39;transaction_date_time\u0026#39;, \u0026#39;hoodie.datasource.write.recordkey.field\u0026#39;: primaryKey, \u0026#39;hoodie.table.name\u0026#39;: targetTableName, \u0026#39;hoodie.datasource.hive_sync.database\u0026#39;: targetDBName, \u0026#39;hoodie.datasource.hive_sync.table\u0026#39;: targetTableName, \u0026#39;hoodie.datasource.hive_sync.enable\u0026#39;: \u0026#39;true\u0026#39;\r}\rpartitionDataConfig = {\r\u0026#39;hoodie.datasource.write.partitionpath.field\u0026#39;: partitionKey, \u0026#39;hoodie.datasource.hive_sync.partition_extractor_class\u0026#39;: \u0026#39;org.apache.hudi.hive.MultiPartKeysValueExtractor\u0026#39;, \u0026#39;hoodie.datasource.hive_sync.partition_fields\u0026#39;: partitionKey\r}\runpartitionDataConfig = {\r\u0026#39;hoodie.datasource.hive_sync.partition_extractor_class\u0026#39;: \u0026#39;org.apache.hudi.hive.NonPartitionedExtractor\u0026#39;, \u0026#39;hoodie.datasource.write.keygenerator.class\u0026#39;: \u0026#39;org.apache.hudi.keygen.NonpartitionedKeyGenerator\u0026#39;\r}\rincrementalConfig = {\r\u0026#39;hoodie.upsert.shuffle.parallelism\u0026#39;: 20, \u0026#39;hoodie.datasource.write.operation\u0026#39;: \u0026#39;upsert\u0026#39;, \u0026#39;hoodie.cleaner.policy\u0026#39;: \u0026#39;KEEP_LATEST_COMMITS\u0026#39;, \u0026#39;hoodie.cleaner.commits.retained\u0026#39;: 10\r}\rinitLoadConfig = {\r\u0026#39;hoodie.bulkinsert.shuffle.parallelism\u0026#39;: 100, \u0026#39;hoodie.datasource.write.operation\u0026#39;: \u0026#39;bulk_insert\u0026#39;\r}\rdeleteDataConfig = {\r\u0026#39;hoodie.datasource.write.payload.class\u0026#39;: \u0026#39;org.apache.hudi.common.model.EmptyHoodieRecordPayload\u0026#39;\r}\rif(hudiStorageType == \u0026#39;MoR\u0026#39;):\rcommonConfig = {**commonConfig, **morConfig}\rlogger.info(\u0026#39;MoR config appended to commonConfig.\u0026#39;)\rcombinedConf = {}\r# HUDI require us to provide a primaryKey. If no primaryKey defined, we will fallback to \u0026#39;glueparquet\u0026#39; format\rif(isPrimaryKey):\rlogger.info(\u0026#39;Going the Hudi way.\u0026#39;)\rif(isTableExists):\rlogger.info(\u0026#39;Incremental load.\u0026#39;)\rinputDyf = glueContext.create_dynamic_frame_from_options(connection_type = \u0026#39;s3\u0026#39;, connection_options = {\u0026#39;paths\u0026#39;: [cdcRawS3TablePath], \u0026#39;groupFiles\u0026#39;: \u0026#39;none\u0026#39;, \u0026#39;recurse\u0026#39;:True}, format = \u0026#39;csv\u0026#39;, format_options={\u0026#39;withHeader\u0026#39;:True}, transformation_ctx = targetTableName)\r# Ensure timestamp is in HUDI timestamp format\rinputDf = inputDyf.toDF().withColumn(\u0026#39;transaction_date_time\u0026#39;, to_timestamp(col(\u0026#39;transaction_date_time\u0026#39;))).withColumn(primaryKey, col(primaryKey).cast(StringType()))\r# ensure only latest updates are kept for each record using descending timestamp order\rw = Window.partitionBy(primaryKey).orderBy(desc(\u0026#39;transaction_date_time\u0026#39;))\rinputDf = inputDf.withColumn(\u0026#39;Rank\u0026#39;,dense_rank().over(w))\rinputDf = inputDf.filter(inputDf.Rank == 1).drop(inputDf.Rank)\routputDf = inputDf.filter(\u0026#34;Op != \u0026#39;D\u0026#39;\u0026#34;).drop(*dropColumnList)\rif outputDf.count() \u0026gt; 0:\rlogger.info(\u0026#39;Upserting data. Updated row count = \u0026#39; + str(outputDf.count()))\rif (isPartitionKey):\rlogger.info(\u0026#39;Writing to partitioned Hudi table.\u0026#39;)\routputDf = outputDf.withColumn(partitionKey,concat(lit(partitionKey+\u0026#39;=\u0026#39;),col(partitionKey)))\rcombinedConf = {**commonConfig, **partitionDataConfig, **incrementalConfig}\routputDf.write.format(\u0026#39;hudi\u0026#39;).options(**combinedConf).mode(\u0026#39;Append\u0026#39;).save(targetPath)\relse:\rlogger.info(\u0026#39;Writing to unpartitioned Hudi table.\u0026#39;)\rcombinedConf = {**commonConfig, **unpartitionDataConfig, **incrementalConfig}\routputDf.write.format(\u0026#39;hudi\u0026#39;).options(**combinedConf).mode(\u0026#39;Append\u0026#39;).save(targetPath)\routputDf_deleted = inputDf.filter(\u0026#34;Op = \u0026#39;D\u0026#39;\u0026#34;).drop(*dropColumnList)\rif outputDf_deleted.count() \u0026gt; 0:\rlogger.info(\u0026#39;Some data got deleted.\u0026#39;)\rif (isPartitionKey):\rlogger.info(\u0026#39;Deleting from partitioned Hudi table.\u0026#39;)\routputDf_deleted = outputDf_deleted.withColumn(partitionKey,concat(lit(partitionKey+\u0026#39;=\u0026#39;),col(partitionKey)))\rcombinedConf = {**commonConfig, **partitionDataConfig, **incrementalConfig, **deleteDataConfig}\routputDf_deleted.write.format(\u0026#39;hudi\u0026#39;).options(**combinedConf).mode(\u0026#39;Append\u0026#39;).save(targetPath)\relse:\rlogger.info(\u0026#39;Deleting from unpartitioned Hudi table.\u0026#39;)\rcombinedConf = {**commonConfig, **unpartitionDataConfig, **incrementalConfig, **deleteDataConfig}\routputDf_deleted.write.format(\u0026#39;hudi\u0026#39;).options(**combinedConf).mode(\u0026#39;Append\u0026#39;).save(targetPath)\relse:\routputDf = inputDf.drop(*dropColumnList)\rif outputDf.count() \u0026gt; 0:\rlogger.info(\u0026#39;Inital load.\u0026#39;)\rif (isPartitionKey):\rlogger.info(\u0026#39;Writing to partitioned Hudi table.\u0026#39;)\routputDf = outputDf.withColumn(partitionKey,concat(lit(partitionKey+\u0026#39;=\u0026#39;),col(partitionKey)))\rcombinedConf = {**commonConfig, **partitionDataConfig, **initLoadConfig}\routputDf.write.format(\u0026#39;hudi\u0026#39;).options(**combinedConf).mode(\u0026#39;Overwrite\u0026#39;).save(targetPath)\relse:\rlogger.info(\u0026#39;Writing to unpartitioned Hudi table.\u0026#39;)\rcombinedConf = {**commonConfig, **unpartitionDataConfig, **initLoadConfig}\routputDf.write.format(\u0026#39;hudi\u0026#39;).options(**combinedConf).mode(\u0026#39;Overwrite\u0026#39;).save(targetPath)\relse:\rif (isPartitionKey):\rlogger.info(\u0026#39;Writing to partitioned glueparquet table.\u0026#39;)\rsink = glueContext.getSink(connection_type = \u0026#39;s3\u0026#39;, path= targetPath, enableUpdateCatalog = True, updateBehavior = \u0026#39;UPDATE_IN_DATABASE\u0026#39;, partitionKeys=[partitionKey])\relse:\rlogger.info(\u0026#39;Writing to unpartitioned glueparquet table.\u0026#39;)\rsink = glueContext.getSink(connection_type = \u0026#39;s3\u0026#39;, path= targetPath, enableUpdateCatalog = True, updateBehavior = \u0026#39;UPDATE_IN_DATABASE\u0026#39;)\rsink.setFormat(\u0026#39;glueparquet\u0026#39;)\rsink.setCatalogInfo(catalogDatabase = targetDBName, catalogTableName = targetTableName)\routputDyf = DynamicFrame.fromDF(inputDf.drop(*dropColumnList), glueContext, \u0026#39;outputDyf\u0026#39;)\rsink.writeFrame(outputDyf)\rexcept BaseException as e:\rlogger.info(\u0026#39;An error occurred while processing table \u0026#39; + targetDBName + \u0026#39;.\u0026#39; + targetTableName + \u0026#39;. Please check the error logs...\u0026#39;)\rprint(e)\rjob.commit() Chọn vào Job Detail: Nhập Name là: glue-hudi-job Chọn Iam Role tương tự như thế này: -GlueLabRole- Chọn Type là Spark\nGlue version: Glue 3.0 - Supports spark 3.1, Scala 2, Python 3.\nChọn Language là Python 3\nDisable Job bookmark\nTất cả còn lại để mặc định.\nMở rộng Advanced Properties\nNhập Job parameters\nNhấn Save và click Run\nĐợi tới khi Succeeded Glue job sẽ tạo HUDI tables ticket_purchase_hist\nStep 2 – Truy vấn HUDI table trong Athena Đảm báo rằng HUDI table được tạo thành công. Click vào Tables\nBạn sẽ thấy new tables là ticket_purchase_hist và database là hudi_sample Truy cập Amazon Athena Console và chọn hudi_sample Database\nSetup nơi lưu kết quả.\nChọn ellipsis và Preview table, ticket_purchase_hist Count ticket_purchase_hist bằng cách chạy query sau select count(1) from hudi_sample.ticket_purchase_hist Kết quả HUDI tables Step 3 – HUDI HUDI configurations Một số configurations:\nAttribute Description hoodie.datasource.write.table.type Chọn COPY_ON_WRITE hoặc MERGE_ON_READ table type. hoodie.datasource.write.recordkey.field Tương tự primary key ở cơ sở dữ liệu quan hệ hoodie.datasource.hive_sync.partition_fields Trường trong bảng dùng để xác định các hive partition columns hoodie.datasource.write.operation Chọn upsert, insert hoặc bulkinsert cho các hành động write hoodie.datasource.read.end.instanttime Thời gian tức thì nạp data Xem thêm các configurations khác Apache Hudi documentation\nStep 4 – Upsert Incremental Changes Ở các bước trước, chúng ta đã tạo một bảng HUDI có tên ticket_purchase_hist, bao gồm toàn bộ bảng nguồn. Bây giờ, hãy tạo một số dữ liệu CDC để có thể cập nhật các thay đổi gia tăng vào bảng HUDI.\nLàm theo các bước ở đây để tạo CDC data - Generate CDC Data\nSau khi CDC data được tạo, đảm bảo rằng cdctask trong DMS cập nhập bản ghi như bên dưới: Chạy Glue job glue-hudi-job để upsert the CDC changestới HUDI table ticket_purchase_hist. Sau khi Glue job chạy thành công. Bạn có thể truy cập Amazon Athena Console để xác minh việc tăng số lượng bản ghi. Step 5 – Chạy Incremental Queries sử dụng Spark SQL Truy cập AWS Glue Console, click Job Chọn gluehudijob. Nhấn Clone job ở Action Copy đoạn code sau.\nimport os\rimport sys\rimport boto3\rfrom awsglue.context import GlueContext\rfrom awsglue.dynamicframe import DynamicFrame\rfrom awsglue.job import Job\rfrom awsglue.utils import getResolvedOptions\rfrom botocore.exceptions import ClientError\rfrom pyspark.sql.session import SparkSession\rfrom pyspark.sql.types import *\rargs = getResolvedOptions(sys.argv, [\u0026#39;JOB_NAME\u0026#39;, \u0026#39;CURATED_BUCKET\u0026#39;])\rspark = SparkSession.builder.config(\r\u0026#39;spark.serializer\u0026#39;,\r\u0026#39;org.apache.spark.serializer.KryoSerializer\u0026#39;).config(\u0026#39;spark.sql.hive.convertMetastoreParquet\u0026#39;,\u0026#39;false\u0026#39;).getOrCreate()\rglueContext = GlueContext(spark.sparkContext)\rjob = Job(glueContext)\rjob.init(args[\u0026#39;JOB_NAME\u0026#39;], args)\rlogger = glueContext.get_logger()\rlogger.info(\u0026#39;Initialization.\u0026#39;)\rglueClient = boto3.client(\u0026#39;glue\u0026#39;)\rlogger.info(\u0026#39;Fetching configuration.\u0026#39;)\rregion = os.environ[\u0026#39;AWS_DEFAULT_REGION\u0026#39;]\rcuratedBucketName = args[\u0026#39;CURATED_BUCKET\u0026#39;]\rif curatedBucketName == None:\rraise Exception(\r\u0026#34;Please input the bucket names in job parameters. Refer: https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-get-resolved-options.html\u0026#34;\r)\rkeys = {\r\u0026#34;dms_sample.ticket_purchase_hist\u0026#34;: {\u0026#34;primaryKey\u0026#34;: \u0026#34;sporting_event_ticket_id\u0026#34;}\r}\rcuratedS3TablePathPrefix = \u0026#39;s3://\u0026#39; + curatedBucketName + \u0026#39;/hudi/\u0026#39;\rhudiDBName = \u0026#39;hudi_sample\u0026#39;\rhudiTableName = \u0026#39;ticket_purchase_hist\u0026#39;\rprimaryKey = \u0026#39;sporting_event_ticket_id\u0026#39;\runpartitionDataConfig = {\r\u0026#39;hoodie.datasource.hive_sync.partition_extractor_class\u0026#39;: \u0026#39;org.apache.hudi.hive.NonPartitionedExtractor\u0026#39;,\r\u0026#39;hoodie.datasource.write.keygenerator.class\u0026#39;: \u0026#39;org.apache.hudi.keygen.NonpartitionedKeyGenerator\u0026#39;\r}\rincrementalConfig = {\r\u0026#39;hoodie.upsert.shuffle.parallelism\u0026#39;: 68,\r\u0026#39;hoodie.datasource.write.operation\u0026#39;: \u0026#39;upsert\u0026#39;,\r\u0026#39;hoodie.cleaner.policy\u0026#39;: \u0026#39;KEEP_LATEST_COMMITS\u0026#39;,\r\u0026#39;hoodie.cleaner.commits.retained\u0026#39;: 10\r}\rall_commits = list(\rmap(\rlambda row: row[0],\rspark.sql(\r\u0026#34;select distinct(_hoodie_commit_time) as commitTime from \u0026#34; + hudiDBName + \u0026#34;.ticket_purchase_hist order by commitTime\u0026#34;\r).limit(50).collect()))\rlogger.info(\u0026#39;Total number of commits are: \u0026#39; + str(len(all_commits)))\rbeginTime = all_commits[len(all_commits) - 2] # commit time we are interested in\r# incrementally query data\rincremental_read_options = {\r\u0026#39;hoodie.datasource.query.type\u0026#39;: \u0026#39;incremental\u0026#39;,\r\u0026#39;hoodie.datasource.read.begin.instanttime\u0026#39;: beginTime,\r}\rincrementalDF = spark.read.format(\u0026#34;hudi\u0026#34;).options(**incremental_read_options). \\\rload(curatedS3TablePathPrefix + hudiDBName + \u0026#39;/\u0026#39; + hudiTableName)\rcommonConfig = {\r\u0026#39;className\u0026#39;: \u0026#39;org.apache.hudi\u0026#39;,\r\u0026#39;hoodie.datasource.hive_sync.use_jdbc\u0026#39;: \u0026#39;false\u0026#39;,\r\u0026#39;hoodie.datasource.write.precombine.field\u0026#39;: \u0026#39;transaction_date_time\u0026#39;,\r\u0026#39;hoodie.datasource.write.recordkey.field\u0026#39;: primaryKey,\r\u0026#39;hoodie.table.name\u0026#39;: hudiTableName + \u0026#39;_incremental\u0026#39;,\r\u0026#39;hoodie.datasource.hive_sync.database\u0026#39;: hudiDBName,\r\u0026#39;hoodie.datasource.hive_sync.table\u0026#39;: hudiTableName + \u0026#39;_incremental\u0026#39;,\r\u0026#39;hoodie.datasource.hive_sync.enable\u0026#39;: \u0026#39;true\u0026#39;\r}\rcombinedConf = { **commonConfig, **unpartitionDataConfig, **incrementalConfig }\rincrementalDF.write.format(\u0026#39;hudi\u0026#39;).options(**combinedConf).mode(\u0026#39;Overwrite\u0026#39;).save(curatedS3TablePathPrefix + \u0026#39;/\u0026#39; + hudiDBName + \u0026#39;/\u0026#39; + hudiTableName + \u0026#39;_incremental\u0026#39;)\rjob.commit() Chọn Job Details Name: incremental-hudi-job IAM role: -GlueLabRole- Click Save and Run Đợi status chuyển Succeeded Truy cập Amazon Athena console và truy vấn table ticket_purchase_hist_incremental Tổng kết Chúng ta đã thực hiện được:\nGlue job để tạo HUDI tables tương ứng với source tables ticket_purchase_hist cho CDC changes Glue job khác để nhận incremental changes và lưu kết quả vào tables ticket_purchase_hist_incremental Truy vấn HUDI tables bằng Amazon Athena "
},
{
	"uri": "//localhost:1313/2-lab1/2.2-lab/",
	"title": "Lab",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Hướng dẫn này giúp bạn hoàn thành lab \u0026ldquo;Phát hiện luồng click chuột bất thường bằng Amazon Managed Service for Apache Flink\u0026rdquo;\nPhân tích lưu lượng truy cập web để hiểu rõ hơn nhằm thúc đẩy các quyết định kinh doanh trước đây được thực hiện bằng cách sử dụng xử lý hàng loạt. Mặc dù hiệu quả nhưng cách tiếp cận này dẫn đến phản ứng chậm trễ đối với các xu hướng mới nổi và hoạt động của người dùng. Có những giải pháp xử lý dữ liệu trong thời gian thực bằng cách sử dụng công nghệ streaming và micro-batching, nhưng việc thiết lập và bảo trì có thể phức tạp. Amazon Managed Service dành cho Apache Flink là dịch vụ được quản lý giúp dễ dàng xác định và phản hồi các thay đổi về hành vi dữ liệu trong thời gian thực.\nSteps: Setup Amazon Analytics Studio Application thông qua CloudFormation stack deployment Tạo real time website traffic sử dụng Amazon Kinesis Data Generator (KDG) Thực hiện real-time Data Analytics Dọn dẹp môi trường Phụ lục: Các tập lệnh Trong Kinesis prelab setup, bạn đã hoàn thành các điều kiện tiên quyết cho bài lab này. Trong bài lab, bạn sẽ tạo quy trình Amazon Managed Service cho Apache Flink.\nSet up Amazon Analytics Studio Application thông qua CloudFormation stack deployment Click vào đây để deploy CloudFormation Stack: Deploy To AWS\rĐiền các tham số , chọn IAM và check vào box \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources.\u0026rdquo;\nStack Sex taoj ra 6 Amazon Kinesis Data Streams trong Amazon Kinesis Console tickerstream – Stream khởi tạo traffic\nclickstream – Nắm bắt số lượng nhấp chuột\nimpressionstream – Số lần hiển thị\nctrstream – bắt tỷ lệ nhấp được tính toán\ndestinationstream – nắm bắt được điểm số bất thường\nanomalydetectionstream – ghi lại các bản ghi có điểm bất thường lớn hơn 2\nCloudFormation Stack cũng sẽ tạo một ứng dụng Amazon Analytics Studio có tên là kda-flink-prelab-RealtimeApplicationNotebook trong tab Amazon Kinesis Application Console. Chúng ta sẽ viết Studio Notebook tương tác trong Apache Zeppelin để phân tích dữ liệu theo thời gian thực. Chạy Ứng dụng Studio bằng cách chọn kda-flink-prelab-RealtimeApplicationNotebook trong tab Studio. Chọn “Run” lần nữa trên màn hình tiếp theo. Tạo lưu lượng truy cập trang web theo thời gian thực bằng Amazon Kinesis Data Generator (KDG) Mở output Amazon CloudFormation console click vào link Amazon Kinesis Data Generator Bắt đầu gửi traffic\n{\u0026#34;browseraction\u0026#34;:\u0026#34;Impression\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;https://www.mysite.com\u0026#34;} {\u0026#34;browseraction\u0026#34;:\u0026#34;Click\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;https://www.mysite.com\u0026#34;} Bạn có thể xem số lượng được gửi đến data stream\nSau 30 giây thì dừng\nThực hiện phân tích dữ liệu thời gian thực Mở Amazon Kinesis Application Console, chọn kda-prelab-template-RealtimeApplicationNotebook. Chọn “Open in Apache Zeppelin”. Tại Apache Zeppelin Console, chọn Create new note. Tên notebook là kda_Interactive_notebook Thực hiện phân tích tương tác theo thời gian thực với luồng dữ liệu Kinesis.\nTạo bảng Flink bằng Truy vấn SQL Flink\nSử dụng truy vấn Flink SQL để chuyển đổi và tạo luồng dữ liệu mới trong thời gian thực\nThực hiện phát hiện bất thường bằng Chức năng do người dùng xác định Flink và kích hoạt email thông báo bất thường trong thời gian thực.\nCác script ở đây.\nNotebook cũng có tại đây. Bạn có thể tải về và imported thông qua Apache Zeppelin console.\nSau đó, bạn có thể mở notebook và chạy từng đoạn một. Sơ đồ luồng Chạy Apache Zeppelin Chạy các scripts tạo bảng User Defined Function (UDF) thực hiện Phát hiện bất thường trong thời gian thực bằng thuật toán Random Cut Forest algorithm Bạn có thể xem dữ liệu thời gian thực từ các lượt truy cập trang web bằng cách chạy truy vấn ở Bước #3 Tạo impressionstream bằng cách lọc messages từ tickerstream\nTạo clickstream bằng cách lọc messages từ tickerstream Tính toán Tỷ lệ nhấp (CTR) và điền vào ctrstream. Bạn có thể xem Tỷ lệ nhấp trong thời gian thực bằng cách thực hiện Bước 7. Sử dụng UDF (RRandom Cut Forest) để tạo điểm bất thường. Populate anomalydetectionstream bằng cách thực hiện step 9 Bây giờ hãy kiểm tra điểm bất thường từ thuật toán Random Cut Forest trong thời gian thực: Bạn sẽ bắt đầu nhận được thông báo trong email của mình khi phát hiện thấy sự bất thường: Nếu bạn không nhận được email thông báo bất thường trong lần thử đầu tiên\nMở lại hai phiên đồng thời của KDG UI trong trình duyệt của bạn.\nTrong phiên đầu tiên, gửi tin nhắn hiển thị với tốc độ một tin nhắn mỗi giây đến dòng mã đánh dấu, nội dung tin nhắn là\n{\u0026#34;browseraction\u0026#34;:\u0026#34;Impression\u0026#34;, \u0026#34;trang web\u0026#34;:https://www.mysite.com\u0026#34;} Trong phiên thứ hai, gửi tin nhắn nhấp chuột với tốc độ năm tin nhắn mỗi giây đến dòng mã đánh dấu, nội dung tin nhắn là {\u0026#34;browseraction\u0026#34;\u0026gt;Click\u0026#34;, \u0026#34;trang web\u0026#34;:https://www.mysite.com\u0026#34;} Dừng gửi tin nhắn sau 30-40 giây.\nBây giờ trên Sổ ghi chép Apache Zeppelin, hãy lặp lại các bước từ 3 đến 10 và bạn sẽ bắt đầu nhận được thông báo qua email từ lần thử thứ hai.\nDọn dẹp tài nguyên Xong khi hoàn thành bài lab. Delete kda-flink-pre-lab "
},
{
	"uri": "//localhost:1313/2-lab1/",
	"title": "Lab. Phát hiện luồng click chuột bất thường bằng Amazon Managed Service for Apache Flink",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Dữ liệu truyền trực tuyến là dữ liệu được tạo ra liên tục bởi hàng nghìn nguồn dữ liệu, thường gửi các bản ghi dữ liệu đồng thời và ở kích thước nhỏ (Kilobyte). Dữ liệu truyền trực tuyến bao gồm nhiều loại dữ liệu như tệp nhật ký do khách hàng tạo bằng ứng dụng web hoặc thiết bị di động của bạn, mua hàng thương mại điện tử, hoạt động của người chơi trong trò chơi, thông tin từ mạng xã hội, sàn giao dịch tài chính hoặc dịch vụ không gian địa lý và đo từ xa từ các thiết bị được kết nối hoặc thiết bị trong trung tâm dữ liệu.\nDữ liệu này cần được xử lý tuần tự và tăng dần trên cơ sở từng bản ghi hoặc trên các cửa sổ thời gian trượt và được sử dụng cho nhiều loại phân tích bao gồm tương quan, tổng hợp, lọc và lấy mẫu. Thông tin thu được từ phân tích như vậy mang lại cho các công ty cái nhìn sâu sắc về nhiều khía cạnh trong hoạt động kinh doanh và khách hàng của họ, chẳng hạn như –việc sử dụng dịch vụ (để đo lường/thanh toán), hoạt động của máy chủ, số lần nhấp vào trang web và vị trí địa lý của thiết bị, con người và hàng hóa vật lý –và cho phép để họ có thể ứng phó kịp thời với những tình huống mới nổi. Ví dụ: doanh nghiệp có thể theo dõi những thay đổi trong cảm nhận của công chúng đối với thương hiệu và sản phẩm của họ bằng cách liên tục phân tích các luồng truyền thông xã hội và phản hồi kịp thời khi cần thiết.\nAWS có rất nhiều công nghệ mạnh mẽ như Kinesis Data Streams, Kinesis Data Firehose, Amazon Managed Service cho Apache Flink và Managed Streaming cho Kafka khi làm việc với dữ liệu truyền trực tuyến. Trong phòng thí nghiệm này, chúng tôi sẽ đề cập đến một số dịch vụ chính này bằng các bài tập thực hành.\n"
},
{
	"uri": "//localhost:1313/6-labdatalakeautomation/6.2-lakeformationlabforapachehuditables/",
	"title": "Lake Formation Lab for Apache Hudi Tables",
	"tags": [],
	"description": "",
	"content": "For our EC2 instances to be able to send session logs to the S3 bucket, we will need to update the IAM Role assigned to the EC2 instance by adding a policy that allows access to S3.\nUpdate IAM Role Go to IAM service management console Click Roles. In the search box, enter SSM. Click on the SSM-Role role. Click Attach policies. In the Search box enter S3. Click the policy AmazonS3FullAccess. Click Attach policy. In the production environment, we will grant stricter permissions to the specified S3 bucket. In the framework of this lab, we use the policy AmazonS3FullAccess for convenience.\nNext, we will proceed to create an S3 bucket to store session logs.\n"
},
{
	"uri": "//localhost:1313/3-ingestionwithdms/3.3-skip-dms-lab/",
	"title": "Option 2: AutoComplete DMS Lab",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Lab trong Data Engineering workshop được thiết kế theo tuần tự. Bài lab này tự động deploy AWS Database Migration Service (AWS DMS) để chúng ta có thể nhanh chóng đến Glue Lab. Nếu muốn hands-on với AWS DMS service. Hãy chọn Option 1: DMS Main Lab\nAutoComplete DMS Chọn \u0026ldquo;Deploy to AWS\u0026rdquo; để deploy stack Deploy To AWS\rStack sẽ hoàn thành các task sau:\nKhởi tạo môi trường cho workshop Tạo DMS subnet group trong VPC Tạo DMS replication instance Tạo source endpoint cho RDS source database Tạo target endpoint cho full data load Tạo target endpoint cho CDC Tạo task thực hiện full migration Tạo task hỗ trợ replication thay đổi. Chọn Parameters: DMSCWRoleCreated: Nếu có role dms-cloudwatch-logs-role thì chọn true, nếu không thì chọn false DMSVPCRoleCreated: Nếu có dms-vpc-role thì chọn true, còn không thì chọn false ServerName: RDS Database Server Name Phía dưới Capabilities, tích checkbox acknowledge the policy và chọn Create stack để tạo.\nStack cần 5-6 phút để hoàn thành. Đợi đến khi \u0026ldquo;CREATE_COMPLETE\u0026rdquo;\nTại thời điểm này, dữ liệu nguồn đã được tải đầy đủ từ cơ sở dữ liệu RDS sang S3 bucket thông qua DMS. Truy cập AWS DMS console , bạn sẽ thấy hai Database migration tasks đã hoàn thành 100%. Nếu không, vui lòng đợi cho đến khi hoàn thành rồi chuyển sang Glue lab "
},
{
	"uri": "//localhost:1313/5-labqueryandvisualize/5.3-athenaandsagemaker/",
	"title": "Athena and SageMaker",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Amazon SageMaker là một nền tảng machine learning toàn diện cho phép bạn xây dựng, đào tạo và triển khai các mô hình machine learning trong AWS. Đây là một dịch vụ có tính mô-đun cao cho phép bạn sử dụng từng thành phần này một cách độc lập với nhau.\nỞ bài này, chúng ta sẽ thực hành:\nCách sử dụng Jupyter notebooks trong SageMaker để tích hợp với data lake bằng Athena Tạo data frames để thao tác với dữ liệu Tạo Amazon SageMaker Notebook Instance Truy cập Amazon Sagemaker, Tại Notebook instances chọn Create notebook instance Nhập các thông tin sau:\nName: datalake-Sagemaker Type: ml.t3.medium Elastic Inference: none. IAM role: Tạo IAM role Any S3 bucket, gán thêm AmazonAthenaFullAccess Chọn Create notebook instance Đợi Trạng thái của note book thành InService. Chọn Open Jupyter Mở notebook interface Kết nối SageMaker Jupyter notebook tới Athena Tại Jupyter notebook tab, Chọn New và chọn conda_python3 Cài đặt PyAthena\n!pip install PyAthena[SQLAlchemy] Làm việc với panda Chạy Command sau đây bằng notebook cell để lấy region hiện tại !aws configure get region Chạy đoạn code sau trong note book:\nfrom sqlalchemy import create_engine\rimport pandas as pd\rs3_staging_dir = \u0026#34;s3://dmslab-student-dmslabs3bucket-xxx/athenaquery/\u0026#34;\rconnection_string = f\u0026#34;awsathena+rest://:@athena.us-east-1.amazonaws.com:443/ticketdata?s3_staging_dir={s3_staging_dir}\u0026#34;\rengine = create_engine(connection_string)\rdf = pd.read_sql(\u0026#39;SELECT * FROM \u0026#34;ticketdata\u0026#34;.\u0026#34;nfl_stadium_data\u0026#34; order by stadium limit 10;\u0026#39;, engine)\rdf Chọn Run và quan sát dataframe, df = pd.read_sql(\u0026#39;SELECT sport, \\\revent_date_time, \\\rhome_team,away_team, \\\rcity, \\\rcount(*) as tickets, \\\rsum(ticket_price) as total_tickets_amt, \\\ravg(ticket_price) as avg_ticket_price, \\\rmax(ticket_price) as max_ticket_price, \\\rmin(ticket_price) as min_ticket_price \\\rFROM \u0026#34;ticketdata\u0026#34;.\u0026#34;sporting_event_ticket_info\u0026#34; \\\rgroup by 1,2,3,4,5 \\\rorder by 1,2,3,4,5 limit 1000;\u0026#39;, engine)\rdf Vẽ biểu đồ import matplotlib.pyplot as plt df.plot(x=\u0026#39;event_date_time\u0026#39;,y=\u0026#39;avg_ticket_price\u0026#39;) "
},
{
	"uri": "//localhost:1313/2-lab1/2.3-labetl/",
	"title": "Lab. Streaming ETL with Glue ",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Trong bài lab này, ta sẽ tìm hiểu cách nhập, xử lý và sử dụng streaming data bằng các dịch vụ serverless của AWS như Kinesis Data Streams, Glue, S3 và Athena. Để mô phỏng đầu vào truyền dữ liệu, chúng tôi sẽ sử dụng Kinesis Data Generator (KDG).\nSetup môi trường Sử dụng DMS Lab Student PreLab CloudFormation để thiết lập môi trường cơ sở hạ tầng hội thảo cốt lõi của bạn. Bỏ qua PreLab tương tự trong phần DMS. Nhấp vào biểu tượng Triển khai lên AWS bên dưới:\nDeploy To AWS\rSet up kinesis stream Mở AWS Kinesis console Chọn “Create data stream” Nhập số liệu như sau: Data stream name: TicketTransactionStreamingData Capacity mode: Provisioned Provisioned shards: 2 Chọn Create data stream Create Table for Kinesis Stream Source in Glue Data Catalog Mở tab AWS Glue console\nTạo database có tên là \u0026ldquo;tickettransactiondatabase\u0026rdquo; Tạo tables có tên là \u0026ldquo;TicketTransactionStreamData\u0026rdquo; ở trong database \u0026ldquo;tickettransactiondatabase\u0026rdquo; Chọn Kinesis làm nguồn, chọn Luồng trong my account để chọn luồng dữ liệu Kinesis, chọn khu vực AWS thích hợp nơi bạn đã tạo luồng, chọn tên luồng là TicketTransactionStreamingData từ danh sách thả xuống, chọn JSON làm định dạng dữ liệu đến, vì chúng ta sẽ gửi JSON payloads từ Kinesis Data Generator theo các bước sau. và nhấp vào Tiếp theo. Để trống schema vì chúng ta sẽ bật tính năng schema detection. Để trống partition indices. Chọn Next Review lại tất cả thông tin và nhấn Create\nChọn vào Table để xem các thuộc tính Tạo và trigger Glue Streaming job Tại mục Data Integration and ETL chọn Glue Studio Chọn Visual with a blank canvas và nhấn Create Chọn Amazon Kinesis từ Source drop down Trong bảng bên phải phía dưới “Data source properties - Kinesis Stream”, cấu hình như sau:\nAmazon Kinesis Source: Data Catalog table Database: tickettransactiondatabase Table: tickettransactionstreamdata Đảm bảo rằng Detect schema được chọn Để tất cả còn lại mặc định Chọn Amazon S3 từ target drop down list Chọn Data target - S3 bucket và cấu hình như sau: Format: Parquet Compression Type: None S3 Target Location: Select Browse S3 and select the “mod-xxx-dmslabs3bucket-xxx” bucket Cuối cùng chọn Job details tab và cấu hình theo như sau: Name: TicketTransactionStreamingJob IAM Role: Select the xxx-GlueLabRole-xxx from the drop down list Type: Spark Streaming Nhấn Save button để tạo job\nKhi thấy Successfully created job ta nhấn Run button để start job\nTrigger streaming data từ Kinesis Data Generator Truy cập Kinesis Data Generator url từ tab setup và đăng nhập. Đảm bảo chọn đúng region. Chọn TicketTransactionStreamingData là target Kinesis stream để Records per second mặc định (100 records per second). Đối với template, nhập NormalTransaction, copy và dán template payload như sau: {\r\u0026#34;customerId\u0026#34;: \u0026#34;{{random.number(50)}}\u0026#34;,\r\u0026#34;transactionAmount\u0026#34;: {{random.number(\r{\r\u0026#34;min\u0026#34;:10,\r\u0026#34;max\u0026#34;:150\r}\r)}},\r\u0026#34;sourceIp\u0026#34; : \u0026#34;{{internet.ip}}\u0026#34;,\r\u0026#34;status\u0026#34;: \u0026#34;{{random.weightedArrayElement({\r\u0026#34;weights\u0026#34; : [0.8,0.1,0.1],\r\u0026#34;data\u0026#34;: [\u0026#34;OK\u0026#34;,\u0026#34;FAIL\u0026#34;,\u0026#34;PENDING\u0026#34;]\r} )}}\u0026#34;,\r\u0026#34;transactionTime\u0026#34;: \u0026#34;{{date.now}}\u0026#34; } Click Send data để trigger transaction streaming data. Tạo Glue Crawler để transformed data Truy cập AWS Glue console Tại AWS Glue menu, chọn Crawlers and click Add crawler\nNhập tên crawler là TicketTransactionParquetDataCrawler, nhấn Next Click vào Add a datasource Chọn S3 và chỉ định path Sau khi thêm datasource, nhấn next\nChọn IAM Role và nhấn Next Chọn prefix là parquet_ cho tables Đăt Crawler Schedule chạy mỗi giờ. Review lại Crawler và Click Create để tạo Crawler Sau khi Crawler tạo xong. Nhấn Run crawler để trigger lần đầu.\nKhi crawler job stop, chuyển đến Glue Data catalog. Đảm bảo rằng parquet_tickettransactionstreamingdata table xuất hiện Click vào parquet_tickettransactionstreamingdata table để xem chi tiết Trigger dữ liệu bất thường từ Kinesis Data Generator(KDG) Mở Kinesis Data Generator, chọn đúng region. Chọn TicketTransactionStreamingData là Kinesis stream đích Template cho record\n{\r\u0026#34;customerId\u0026#34;: \u0026#34;{{random.number(50)}}\u0026#34;,\r\u0026#34;transactionAmount\u0026#34;: {{random.number(\r{\r\u0026#34;min\u0026#34;:10,\r\u0026#34;max\u0026#34;:150\r}\r)}},\r\u0026#34;sourceIp\u0026#34; : \u0026#34;221.233.116.256\u0026#34;,\r\u0026#34;status\u0026#34;: \u0026#34;{{random.weightedArrayElement({\r\u0026#34;weights\u0026#34; : [0.8,0.1,0.1],\r\u0026#34;data\u0026#34;: [\u0026#34;OK\u0026#34;,\u0026#34;FAIL\u0026#34;,\u0026#34;PENDING\u0026#34;]\r} )}}\u0026#34;,\r\u0026#34;transactionTime\u0026#34;: \u0026#34;{{date.now}}\u0026#34; } Click send data Sử dụng Athena để truy vấn dữ liệu Mở AWS Athena console Chọn AwsDataCatalog làm data source và tickettransactiondatabase là database Sử dụng các truy vấn sau để xem dữ liệu\nSELECT count(*) as numberOfTransactions, sourceip\rFROM \u0026#34;tickettransactiondatabase\u0026#34;.\u0026#34;parquet_tickettransactionstreamingdata\u0026#34; WHERE ingest_year=\u0026#39;2024\u0026#39;\rAND cast(ingest_year as bigint)=year(now())\rAND cast(ingest_month as bigint)=month(now())\rAND cast(ingest_day as bigint)=day_of_month(now())\rAND cast(ingest_hour as bigint)=hour(now())\rGROUP BY sourceip\rOrder by numberOfTransactions DESC; "
},
{
	"uri": "//localhost:1313/6-labdatalakeautomation/6.3-lakeformationlabforapacheicebergtables/",
	"title": "Lake Formation Lab for Apache Iceberg Tables",
	"tags": [],
	"description": "",
	"content": "For our EC2 instances to be able to send session logs to the S3 bucket, we will need to update the IAM Role assigned to the EC2 instance by adding a policy that allows access to S3.\nUpdate IAM Role Go to IAM service management console Click Roles. In the search box, enter SSM. Click on the SSM-Role role. Click Attach policies. In the Search box enter S3. Click the policy AmazonS3FullAccess. Click Attach policy. In the production environment, we will grant stricter permissions to the specified S3 bucket. In the framework of this lab, we use the policy AmazonS3FullAccess for convenience.\nNext, we will proceed to create an S3 bucket to store session logs.\n"
},
{
	"uri": "//localhost:1313/3-ingestionwithdms/",
	"title": "Nhập data với DMS",
	"tags": [],
	"description": "",
	"content": "Giới thiệu AWS Database Migration Service (AWS DMS) giúp bạn di chuyển cơ sở dữ liệu sang AWS một cách nhanh chóng và an toàn. Cơ sở dữ liệu nguồn vẫn hoạt động đầy đủ trong quá trình di chuyển, giảm thiểu thời gian ngừng hoạt động đối với các ứng dụng dựa trên cơ sở dữ liệu. AWS Database Migration Service có thể di chuyển dữ liệu của bạn đến và đi từ các cơ sở dữ liệu thương mại và nguồn mở được sử dụng rộng rãi nhất.\nAWS Database Migration Service hỗ trợ di chuyển đồng nhất như Oracle sang Oracle cũng như di chuyển không đồng nhất giữa các nền tảng cơ sở dữ liệu khác nhau, chẳng hạn như Oracle hoặc Microsoft SQL Server sang Amazon Aurora. Với AWS Database Migration Service, bạn cũng có thể liên tục sao chép dữ liệu với độ trễ thấp từ bất kỳ nguồn được hỗ trợ nào sang bất kỳ mục tiêu được hỗ trợ nào. Ví dụ: bạn có thể sao chép từ nhiều nguồn sang Amazon Simple Storage Service (Amazon S3) để xây dựng giải pháp hồ dữ liệu có tính sẵn sàng cao và có khả năng mở rộng. Bạn cũng có thể hợp nhất cơ sở dữ liệu vào kho dữ liệu quy mô petabyte bằng cách truyền dữ liệu tới Amazon Redshift.\nCác điều hướng của task\nDMS lab có 3 lựa chọn:\nNếu bạn muốn có trải nghiệm thực hành chuyên sâu về DMS (Dịch vụ di chuyển dữ liệu), trước tiên hãy chạy phòng thí nghiệm của người hướng dẫn để mô phỏng môi trường cơ sở dữ liệu quan hệ tại chỗ, sau đó là phòng thí nghiệm dành cho sinh viên để tạo cơ sở hạ tầng di chuyển dữ liệu cần thiết trong AWS . Phòng thí nghiệm chính sẽ giúp bạn thực hiện di chuyển dữ liệu thực tế từ cơ sở dữ liệu quan hệ sang kho dữ liệu trong AWS. Nếu bạn muốn bắt đầu với Glue ETL và bỏ qua phần thực hành DMS hoàn toàn, vui lòng chạy phòng thí nghiệm tự động hoàn thành DMS. Tự động hoàn thành có thể mất từ 15 đến 20 phút để cung cấp tất cả dữ liệu trong lớp thô của hồ dữ liệu S3 tập trung và bạn sẽ sẵn sàng tìm hiểu sâu về chuyển đổi dữ liệu bằng Glue ETL. Nếu bạn không muốn lab dịch vụ DMS, có thể sao chép trực tiếp dữ liệu thô sang S3 bằng Tùy chọn 3: Bỏ qua DMS Lab. Hạn chế của tùy chọn này là bạn không thể sử dụng DMS để tạo dữ liệu gia tăng nhằm kiểm tra tính năng bookmark của Glue. "
},
{
	"uri": "//localhost:1313/3-ingestionwithdms/3.2-private-instance/",
	"title": "Option 3: Skip DMS Lab",
	"tags": [],
	"description": "",
	"content": "Steps Mở AWS CloudShell Copy data từ staging Amazon S3 bucket đến S3 bucket của chúng ta. Kiểm chứng data Kiến trúc. Cơ sở dữ liệu RDS Postgres được sử dụng làm nguồn bán vé cho các sự kiện thể thao. Nó lưu trữ thông tin giao dịch về giá bán vé cho những người được chọn và chuyển quyền sở hữu vé với các bảng bổ sung để biết chi tiết sự kiện. AWS Database Migration Service (DMS) được sử dụng để tải toàn bộ dữ liệu từ nguồn Amazon RDS sang bộ chứa Amazon S3.\nTrước khi Glue lab bắt đầu, ta có thể chọn bỏ qua quá trình DMS data migration. Nếu vậy, hãy sao chép trực tiếp dữ liệu nguồn vào bộ chứa S3 của ta.\nTrong bài lab hôm nay, ta sẽ sao chép dữ liệu từ bộ chứa S3 tập trung vào tài khoản AWS của mình, thu thập dữ liệu bằng trình thu thập thông tin AWS Glue để tạo siêu dữ liệu, chuyển đổi dữ liệu bằng AWS Glue, truy vấn dữ liệu và tạo Chế độ xem bằng Athena và cuối cùng là xây dựng dashboard với Amazon QuickSight.\nOpen AWS CloudShell Mở AWS CloudShell\nCopy data từ staging Amazon S3 bucket đến S3 bucket của chúng ta. aws s3 cp --recursive --copy-props none s3://aws-dataengineering-day.workshop.aws/data/ s3://\u0026lt;YourBucketName\u0026gt;/tickets/ Data sau đây sẽ được copy đến s3 bucket của chúng ta: Xác minh dữ liệu Mở S3 console và xem dữ liệu được copy từ CloudShell Sử dụng S3 select để truy vấn s3 data Next Step. Tiếp theo, Chúng ta sẽ sử hoàn thành bài lab với AWS Glue\nExtract, Transform and Load Data Lake with AWS Glue "
},
{
	"uri": "//localhost:1313/4-transformingdatawithglue/",
	"title": "Lab: Transforming data with Glue",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Trong bài lab này, chúng ta sẽ tìm hiểu về AWS Glue, một dịch vụ tích hợp dữ liệu phi máy chủ giúp khám phá, chuẩn bị, di chuyển và tích hợp dữ liệu từ nhiều nguồn để phân tích, học máy (ML) và phát triển ứng dụng dễ dàng hơn. Bạn có thể sử dụng trình thu thập thông tin để điền vào Danh mục dữ liệu AWS Glue các bảng. Đây là phương pháp chính được hầu hết người dùng AWS Glue sử dụng. Trình thu thập thông tin có thể thu thập dữ liệu nhiều kho dữ liệu trong một lần chạy. Sau khi hoàn tất, trình thu thập thông tin sẽ tạo hoặc cập nhật một hoặc nhiều bảng trong Danh mục dữ liệu của bạn. Các công việc trích xuất, chuyển đổi và tải (ETL) mà bạn xác định trong AWS Glue sử dụng các bảng Danh mục dữ liệu này làm nguồn và mục tiêu. Công việc ETL đọc từ và ghi vào kho dữ liệu được chỉ định trong bảng Danh mục dữ liệu nguồn và đích.\nYêu cầu: Note: Bạn cần hoàn thành DMS Lab để thực hiện bài lab này\nTổng kết Trong bài lab này, bạn sẽ thực hiện các task sau. Bạn có thể chọn chỉ hoàn thành Data Validation and ETL để chuyển sang bài lab tiếp theo nơi có thể truy vấn các bảng bằng Amazon Athena và Visualize bằng Amazon Quciksight.\nData Validation and ETL Incremental Data Processing with Hudi Glue Job Bookmark (Optional) Glue Workflows (Optional) "
},
{
	"uri": "//localhost:1313/6-labdatalakeautomation/6.4-lakeformationlabordeltatables/",
	"title": "Lake Formation Lab for Delta Tables",
	"tags": [],
	"description": "",
	"content": "For our EC2 instances to be able to send session logs to the S3 bucket, we will need to update the IAM Role assigned to the EC2 instance by adding a policy that allows access to S3.\nUpdate IAM Role Go to IAM service management console Click Roles. In the search box, enter SSM. Click on the SSM-Role role. Click Attach policies. In the Search box enter S3. Click the policy AmazonS3FullAccess. Click Attach policy. In the production environment, we will grant stricter permissions to the specified S3 bucket. In the framework of this lab, we use the policy AmazonS3FullAccess for convenience.\nNext, we will proceed to create an S3 bucket to store session logs.\n"
},
{
	"uri": "//localhost:1313/2-lab1/2.4-kinesis/",
	"title": "Real-time Streaming with Kinesis",
	"tags": [],
	"description": "",
	"content": "Real-time Streaming with Kinesis Nếu bạn muốn có thêm các lai lab về Kinesis, truy cập Real-time Streaming with Kinesis để xem nhiều hơn\n"
},
{
	"uri": "//localhost:1313/2-lab1/2.5-msk/",
	"title": "Clickstream Analytics using MSK",
	"tags": [],
	"description": "",
	"content": "Clickstream Analytics using MSK Trong workshop này, mục tiêu chung của chúng ta là trực quan hóa và phân tích hiệu suất của các sản phẩm khác nhau trong trang web thương mại điện tử bằng cách nhập, chuyển đổi và phân tích dữ liệu luồng nhấp chuột theo thời gian thực bằng cách sử dụng dịch vụ AWS cho Apache Kafka (Amazon MSK), Apache Flink (Dữ liệu Kinesis Analytics cho các ứng dụng Java) và Elaticsearch (Amazon Elaticsearch). Clickstream Analytics using MSK để tìm hiểu thêm.\nKiến trúc cấp cao sẽ trông như thế này:\nTrước tiên, chúng ta sẽ sử dụng trình tạo dữ liệu để tạo thông báo Clickstream tới một topic (ExampleTopic) trong cụm Amazon MSK lưu trữ Apache kafka. Sau đó, chúng ta sẽ cấu hình và khởi động Kinesis Data Analytics cho Ứng dụng Java bằng cách sử dụng công cụ Apache Flink, một dịch vụ Apache Flink được quản lý từ AWS. Công cụ này sẽ đọc các sự kiện từ chủ đề exampleTopic trong Amazon MSK, xử lý và tổng hợp nó, sau đó gửi các sự kiện tổng hợp ( Analytics) cho cả hai topic trong Amazon MSK và Amazon Elasticsearch. Sau đó, chúng tôi sẽ sử dụng các thông báo đó từ Amazon MSK để minh họa cách người tiêu dùng có thể nhận được phân tích luồng nhấp chuột theo thời gian thực. Ngoài ra, chúng tôi sẽ tạo trực quan hóa Kibana và bảng điều khiển Kibana để trực quan hóa phân tích luồng nhấp chuột theo thời gian thực.\nChúng tôi sẽ cung cấp các tài nguyên cần thiết cho bài lab thông qua CloudFormation.\nTemplate sẽ trông như sau: The stack sẽ gồm:\nA VPC với 1 Public subnet và 3 Private subnets 1 Public instance that hosts a schema registry service, a producer and consumer. 1 Amazon Elasticsearch cluster. 1 Amazon KDA for Java application. 1 Amazon MSK cluster. Trong bài lab này, chúng ta sẽ ssh vào EC2 instance\nKafkaClientEC2Instance\nClickstream Analytics using MSK to follow complete lab.\n"
},
{
	"uri": "//localhost:1313/5-labqueryandvisualize/",
	"title": "Lab: Query and Visualize",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Bài lab này giới thiệu về AWS Glue, Amazon Athena và Amazon QuickSight. AWS Glue là danh mục dữ liệu và dịch vụ ETL được quản lý toàn phần; Amazon Athena cung cấp khả năng chạy các truy vấn đặc biệt trên dữ liệu trong kho dữ liệu của bạn; và Amazon QuickSight cung cấp khả năng trực quan hóa dữ liệu bạn nhập.\nDưới đây là các bước của bài lab:\nAthena and QuickSight Athena Federated query Athena and SageMaker (Optional) "
},
{
	"uri": "//localhost:1313/6-labdatalakeautomation/",
	"title": "Lab: Data Lake Automation",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Trong bài lab này, chúng ta sẽ cùng tìm hiểu về AWS Lake Formation một dịch vụ giúp bạn dễ dàng thiết lập data-lake an toàn trong vài ngày cũng như Athena để truy vấn dữ liệu bạn nhập vào hồ dữ liệu của mình. Yêu cầu Hoàn thành Ingestion with DMS và Transforming data with Glue ETL labs.\nCông việc hoàn thành trong bài lab này: Trong bài này, chúng ta sẽ:\nTạo JDBC connection to RDS trong AWS Glue Tạo AWS Lake Formation IAM Role Thêm Administrator trong Lake Formation và bắt đầu tạo workflows sử dụng Blueprints Xem các thành phần của Glue Workflow được tạo bởi Lake Formation Kiểm tra kết quả workflow trong Athena Cấp quyền kiểm soát quyền truy cập chi tiết cho người dùng Data Lake Xác minh quyền sử dụng Athena "
},
{
	"uri": "//localhost:1313/7-bonuslabgluedatabrew/",
	"title": "Bonus Lab: Glue DataBrew",
	"tags": [],
	"description": "",
	"content": "We will take the following steps to delete the resources we created in this exercise.\nDelete EC2 instance Go to EC2 service management console\nClick Instances. Select both Public Linux Instance and Private Windows Instance instances. Click Instance state. Click Terminate instance, then click Terminate to confirm. Go to IAM service management console\nClick Roles. In the search box, enter SSM. Click to select SSM-Role. Click Delete, then enter the role name SSM-Role and click Delete to delete the role. Click Users. Click on user Portfwd. Click Delete, then enter the user name Portfwd and click Delete to delete the user. Delete S3 bucket Access System Manager - Session Manager service management console.\nClick the Preferences tab. Click Edit. Scroll down. In the section S3 logging. Uncheck Enable to disable logging. Scroll down. Click Save. Go to S3 service management console\nClick on the S3 bucket we created for this lab. (Example: lab-fcj-bucket-0001 ) Click Empty. Enter permanently delete, then click Empty to proceed to delete the object in the bucket. Click Exit. After deleting all objects in the bucket, click Delete\nEnter the name of the S3 bucket, then click Delete bucket to proceed with deleting the S3 bucket. Delete VPC Endpoints Go to VPC service management console Click Endpoints. Select the 4 endpoints we created for the lab including SSM, SSMMESSAGES, EC2MESSAGES, S3GW. Click Actions. Click Delete VPC endpoints. In the confirm box, enter delete.\nClick Delete to proceed with deleting endpoints. Click the refresh icon, check that all endpoints have been deleted before proceeding to the next step.\nDelete VPC Go to VPC service management console\nClick Your VPCs. Click on Lab VPC. Click Actions. Click Delete VPC. In the confirm box, enter delete to confirm, click Delete to delete Lab VPC and related resources.\n"
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]