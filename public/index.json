[
{
	"uri": "//localhost:1313/",
	"title": "Data Engineering Immersion Day",
	"tags": [],
	"description": "",
	"content": "Data Engineering Immersion Day Data Engineering Immersion Day là gì Đây là một bài hands-on labs có chứa các module tập trung vào việc sinh, nhập, load, khám phá và sử dụng dữ liệu trong data lake trên AWS\nLợi ích Data Engineering Immersion Day Data Engineering Immersion day cung cấp thời gian hands-on trên các dịch vụ phân tích dữ liệu như Amazon Kinesis Services để truyển tải và phân tích dữ liệu, AWS Data Migration service, AWS Glue, Amazon Athena, Amazon Quicksight. Data Engineering Immersion day giúp xây dựng serverless data lake trên nền tảng đám mây.\nContent Introduction Preparation Connect to EC2 instance Manage session logs Port Forwarding Clean up resources "
},
{
	"uri": "//localhost:1313/3-ingestionwithdms/3.1-dms-migration-lab/",
	"title": " Option1: DMS Migration Lab",
	"tags": [],
	"description": "",
	"content": "DMS Migration Lab Steps Giới thiệu Tạo Subnet Group Tạo Replication Instance Tạo DMS Source Endpoint Tạo Target Endpoint Tạo 1 task để bắt đầu full copy Tạo CDC endpoint để replicate thay đổi Tạo task để replicate liên tục Giới thiệu Bài lab này sẽ giúp bạn hiểu rõ hơn về AWS Database Migration Service(AWS DMS). Bạn sẽ di chuyển dữ liệu từ cơ sở dữ liệu Amazon Relational Database Service (Amazon RDS) Postgres hiện có sang Amazon Simple Storage Service (Amazon S3) bucket.\nLink GitHub của lab - https://github.com/aws-samples/data-engineering-for-aws-immersion-day\nTạo Subnet group Tại DMS console , chọn Subnet Groups và Create subnet group. Tại Name textbox: dms-lab-subnet-grp Description textbox: Replication instance for production data system VPC: Chọn *-dmslstudv1 Chọn subnets và click Add Chọn Create subnet group\nTại DMS console, subnet group displays Complete Tạo Replication Instance Tại DMS console, chọn Replication instances để tạo replication instance mới. Name: DMS-Replication-Instance\nDescription: DMS Replication Instance\nInstance class: dms.t3.medium\nChọn engine version mới nhất\nHigh Availability: Dev or test workload (Single-AZ)\nVPC: dmslstudv1 Chọn Advanced để mở rộng\nChọn security group là sgdefault Tất cả các trường còn lại mặc định\nDMS console hiển thị trạng thái tạo instance Tạo MDS Source Endpoint Tại DMS console, chọn Endpoints để tạo source Endpoint Chọn Source Endpoint\nEndpoint identifier: rds-source-endpoint\nSource engine: PostgreSQL\nAccess to Endpoint database: Provide access information manually\nServer name: RDS-Server-Name Port: 5432\nSSL mode: none\nUser name: adminuser\nPassword: admin123\nDatabase name: sportstickets\nTất cả còn lại để mặc định, rồi click tạo endpoint. Khi sẵn sàng, trạng thái sẽ chuyển sang active Kiểm tra lại replication instance Chọn source endpoint và nhấn Test connection Click Run test. Nếu thành công sẽ có thông báo \u0026ldquo;Connection tested successfully\u0026rdquo; xuất hiện. Tạo Target Endpoint Tại DMS console, Chọn Endpoint để tạo target Endpoint Endpoint type: Target endpoint Endpoint identifier: s3-target-endpoint Target engine: Amazon S3 Service access role ARN: Copy và Past DMSLabRoleS3 ARN Bucket name: paste S3 Bucket Name Bucket folder: tickets Mở rộng phần: Endpoint settings\nChọn Use endpoint connection checkbox, điền addColumnName=true trong Extra connection attributes box Mở rộng Test endpoint connection (optional). chọn VPC.\nChọn Run test. Bước này kết nối với source database. Nếu thành công sẽ hiển thị thông báo \u0026ldquo;Connection tested successfully\u0026rdquo; Chọn Create Endpoint. Khi đã sẵn sàng, trạng thái endpoint sẽ chuyển thành active Tạo task initial full copy Tại DMS console, chọn Database Migration Tasks. Chọn Create Task.\nTask name: dms-full-dump-task Chọn Replication instance Chọn Source endpoint Chọn Target endpoint Migration type: Migrate existing data. Mở rộng Task Settings Chọn Turn on CloudWatch logs checkbox Tại Table Mappings Chọn Add new selection rule và chọn Enter a Schema tại Schema field Tại Source name: dms_sample Để tất cả field còn lại mặc định. Chọn Create task. Task sẽ được tạo và tự động start\nChọn task và xem chi tiết. Khi hoàn thành, task console hiển thị 100% progress Mở S3 console và xem data được copy bởi DMS Review data bằng S3 select\nTạo CDC endpoint để replicate các thay đổi diễn ra Tại DMS console, chọn Endpoints Nhấn Create endpoint\nEndpoint type: Target\nEndpoint identifier: rds-cdc-endpoint\nTarget engine: Amazon S3\nService Access Role ARN: DMSLabRoleS3\nBucket name: Chọn S3 Bucketname\nBucket folder: cdc Mở rộng phần Endpoint settings\nTích vào checkbox Use endpoint connection attributes và nhập addColumnName=true. Thuộc tính này bao gồm tên cột từ dữ liệu nguồn. Mở rộng phần Test endpoint connection (optional), chọn VPC name.\nClick Run test. Nếu thành công sẽ hiển thị thông báo \u0026ldquo;Connection tested successfully\u0026rdquo;.\nChọn Create endpoint Khi sẵn sàng, endpoint status chuyển sang active.\nTạo task replication liên tục. Tại DMS console, chọn Database Migration Tasks Chọn Create Task Task Identifier: cdctask Chọn Replication instance Chọn Source endpoint Chọn Target endpoint: rds-cdc-endpoint Chọn Migration type: Replicate data changes only. Trong Task Settings, Chọn Turn on CloudWatch logs checkbox Chuyển đến Table Mappings\nChọn ** Add new selection rule ** và Chọn Enter a Schema tại Schema field\nTại Source name, chọn dms_sample. Tất cả còn lại để mặc định Chọn Create task. Task sẽ được tạo và tự động chạy. Chúng ta có thể thấy trạng thái là Replication ongoing. Đợi 5 đến 10 phút để CDC data ánh xạ RDS postgres database\nChọn CDC task để xem chi tiết, xem phần Table statistics: Mở S3 console và xem CDC data được copied từ DMS\nChọn 1 file và sử dụng S3 Select "
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Kiến trúc của bài lab Nhập dữ liệu từ RDS(Postgres) vào S3 bằng AWS Database Migration Service Chuyển đổi dữ liệu với các service Glue và lưu lại vào S3 Khám phá dữ liệu bằng các công cụ truy vấn và trực quan hóa dữ liệu. Áp dụng Machine Learning sử dụng dịch vụ SageMaker "
},
{
	"uri": "//localhost:1313/2-lab1/2.1-prepare/",
	"title": "Phát hiện luồng click chuột bất thường bằng Amazon Managed Service for Apache Flink",
	"tags": [],
	"description": "",
	"content": "Các bước thực hiện Giới thiệu Deploy CloudFormation Stack Thiết lập Amazon Kinesis Data Generator Thiết lập Email và SMS Subscription Xem xét AWS Lambda Anomaly function Phụ lục: CloudFormation Template Giới thiệu Phần này sẽ là phần thiết lập môi trường cho bài lab.\nSau khi deploy CloudFormation template Kinesis_PreLab.yaml ta sẽ được kiến trúc sau:\nCloudFormation template sẽ tạo các resource sau:\n2 S3 buckets: dùng để lưu trữ dữ liệu thô và dữ liệu đã được xử lý 1 Lambda function: Function này sẽ được trigger khi phát hiện bất thường. Amazon SNS topic: Lamda function sẽ publish tới topic này khi phát hiện click bất thường. Amazon Cognito User credentials: Sử dụng để login vào Kinesis Data Generator để gửi bản ghi đến Kinesis Data Firehose. CloudFormation Stack Deployment Click vào đây để deploy CloudFormation Stack: Deploy To AWS\rNhập các tham số vào trong form:\nĐiền các thông số trong form: Username: Username để đăng nhập vào Kinesis Data Generator Password: Mật khẩu để đăng nhập Kinesis Data Generator. Email: Email để nhận thông báo. SNS topic sẽ gửi mail xác nhận. SMS: Số điện thoại nhận thông báo từ SNS. Ở phần cuối, check vào box marked \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources\u0026rdquo;. Chọn Create. CloudFormation chuyển hướng bạn đến ngăn xếp hiện có của bạn. Sau vài phút, kinesis-pre-lab hiển thị trạng thái CREATE_COMPLETE. Trong khi stack runs, ta sẽ nhân được email như sau: Xác nhập subscription Khi stack deployed, click Outputs để xem thêm thông tin:\nKinesisDataGeneratorUrl: Đây là Kinesis Data Generator (KDG) URL RawBucketName: Tên bucket lưu raw data từ Kinesis Data Generator (KDG) ProcessedBucketName: Tên bucket lưu transformed data Bạn đã hoàn thành việc triển khai CloudFormation.\nKhởi tạo Amazon Kinesis Data Generator (KDG) Tại tab Outputs. Click vào Kinesis Data Generator URL.\nKDG đơn giản hóa nhiệm vụ tạo dữ liệu và gửi dữ liệu tới Amazon Kinesis. Công cụ này cung cấp giao diện người dùng thân thiện với người dùng chạy trực tiếp trong trình duyệt của bạn. Với KDG, bạn có thể thực hiện các tác vụ sau:\nTạo template cho bản ghi trong các trường hợp cụ thể . Tạo dữ liệu cho template với dữ liệu cố định hoặc dữ liệu ngẫu nhiên. Lưu lại template tương lai. Liên tục gửi hàng nghìn bản ghi mỗi giây tới Amazon Kinesis data stream hoặc Firehose delivery stream. Kiểm tra Cognito User trong Kinesis Data Generator.\nClick KinesisDataGeneratorUrl trong Outputs tab Đăng nhập bằng username và password nhập vào CloudFormation console Sau khi đăng nhập, bạn sẽ thấy bảng điều khiển Kinesis Data Generator. Cần thiết lập một số template để giả lập dòng nhấp chuột Tạo các template sau nhưng chưa nhấp vào Gửi dữ liệu, chúng ta sẽ thực hiện việc đó sau. Sao chép phần tô sáng tên tab bằng chữ in đậm và giá trị dưới dạng chuỗi JSON, tham khảo ảnh chụp màn hình:\nSchema Discovery Payload {\u0026#34;browseraction\u0026#34;:\u0026#34;DiscoveryKinesisTest\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;yourwebsiteurl.domain.com\u0026#34;} Click Payload {\u0026#34;browseraction\u0026#34;:\u0026#34;Click\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;yourwebsiteurl.domain.com\u0026#34;} Impression Payload {\u0026#34;browseraction\u0026#34;:\u0026#34;Impression\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;yourwebsiteurl.domain.com\u0026#34;} Amazon Kinesis Data Generator sẽ trông như thế này Xác thực email và SMS Subscription Tại Amazon SNS, chọn Topics để xem. Click vào topic để xem chi tiết: AWS Lambda function Chọn vào Amazon Lamba để xem các lambda function mà CloudFormation đã tạo cho ta: Chọn vào lambda function để xem chi tiết: Tại thời điểm này, chúng ta đã có tất cả các thành phần cần thiết để làm lab.\n"
},
{
	"uri": "//localhost:1313/4-transformingdatawithglue/4.1-updateiamrole/",
	"title": "Update IAM Role",
	"tags": [],
	"description": "",
	"content": "For our EC2 instances to be able to send session logs to the S3 bucket, we will need to update the IAM Role assigned to the EC2 instance by adding a policy that allows access to S3.\nUpdate IAM Role Go to IAM service management console Click Roles. In the search box, enter SSM. Click on the SSM-Role role. Click Attach policies. In the Search box enter S3. Click the policy AmazonS3FullAccess. Click Attach policy. In the production environment, we will grant stricter permissions to the specified S3 bucket. In the framework of this lab, we use the policy AmazonS3FullAccess for convenience.\nNext, we will proceed to create an S3 bucket to store session logs.\n"
},
{
	"uri": "//localhost:1313/4-transformingdatawithglue/4.2-creates3bucket/",
	"title": "Create S3 Bucket",
	"tags": [],
	"description": "",
	"content": "In this step, we will create an S3 bucket to store session logs sent from EC2 instances.\nCreate S3 Bucket Access S3 service management console Click Create bucket. At the Create bucket page. In the Bucket name field, enter the bucket name lab-yourname-bucket-0001 In the Region section, select Region you are doing the current lab. The name of the S3 bucket must not be the same as all other S3 buckets in the system. You can substitute your name and enter a random number when generating the S3 bucket name.\nScroll down and click Create bucket. When we created the S3 bucket we did Block all public access so our EC2 instances won\u0026rsquo;t be able to connect to S3 via the internet. In the next step, we will configure the S3 Gateway Endpoint feature to allow EC2 instances to connect to the S3 bucket via the VPC\u0026rsquo;s internal network.\n"
},
{
	"uri": "//localhost:1313/2-lab1/2.2-lab/",
	"title": "Lab",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Hướng dẫn này giúp bạn hoàn thành lab \u0026ldquo;Phát hiện luồng click chuột bất thường bằng Amazon Managed Service for Apache Flink\u0026rdquo;\nPhân tích lưu lượng truy cập web để hiểu rõ hơn nhằm thúc đẩy các quyết định kinh doanh trước đây được thực hiện bằng cách sử dụng xử lý hàng loạt. Mặc dù hiệu quả nhưng cách tiếp cận này dẫn đến phản ứng chậm trễ đối với các xu hướng mới nổi và hoạt động của người dùng. Có những giải pháp xử lý dữ liệu trong thời gian thực bằng cách sử dụng công nghệ streaming và micro-batching, nhưng việc thiết lập và bảo trì có thể phức tạp. Amazon Managed Service dành cho Apache Flink là dịch vụ được quản lý giúp dễ dàng xác định và phản hồi các thay đổi về hành vi dữ liệu trong thời gian thực.\nSteps: Setup Amazon Analytics Studio Application thông qua CloudFormation stack deployment Tạo real time website traffic sử dụng Amazon Kinesis Data Generator (KDG) Thực hiện real-time Data Analytics Dọn dẹp môi trường Phụ lục: Các tập lệnh Trong Kinesis prelab setup, bạn đã hoàn thành các điều kiện tiên quyết cho bài lab này. Trong bài lab, bạn sẽ tạo quy trình Amazon Managed Service cho Apache Flink.\nSet up Amazon Analytics Studio Application thông qua CloudFormation stack deployment Click vào đây để deploy CloudFormation Stack: Deploy To AWS\rĐiền các tham số , chọn IAM và check vào box \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources.\u0026rdquo;\nStack Sex taoj ra 6 Amazon Kinesis Data Streams trong Amazon Kinesis Console tickerstream – Stream khởi tạo traffic\nclickstream – Nắm bắt số lượng nhấp chuột\nimpressionstream – Số lần hiển thị\nctrstream – bắt tỷ lệ nhấp được tính toán\ndestinationstream – nắm bắt được điểm số bất thường\nanomalydetectionstream – ghi lại các bản ghi có điểm bất thường lớn hơn 2\nCloudFormation Stack cũng sẽ tạo một ứng dụng Amazon Analytics Studio có tên là kda-flink-prelab-RealtimeApplicationNotebook trong tab Amazon Kinesis Application Console. Chúng ta sẽ viết Studio Notebook tương tác trong Apache Zeppelin để phân tích dữ liệu theo thời gian thực. Chạy Ứng dụng Studio bằng cách chọn kda-flink-prelab-RealtimeApplicationNotebook trong tab Studio. Chọn “Run” lần nữa trên màn hình tiếp theo. Tạo lưu lượng truy cập trang web theo thời gian thực bằng Amazon Kinesis Data Generator (KDG) Mở output Amazon CloudFormation console click vào link Amazon Kinesis Data Generator Bắt đầu gửi traffic\n{\u0026#34;browseraction\u0026#34;:\u0026#34;Impression\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;https://www.mysite.com\u0026#34;} {\u0026#34;browseraction\u0026#34;:\u0026#34;Click\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;https://www.mysite.com\u0026#34;} Bạn có thể xem số lượng được gửi đến data stream\nSau 30 giây thì dừng\nThực hiện phân tích dữ liệu thời gian thực Mở Amazon Kinesis Application Console, chọn kda-prelab-template-RealtimeApplicationNotebook. Chọn “Open in Apache Zeppelin”. Tại Apache Zeppelin Console, chọn Create new note. Tên notebook là kda_Interactive_notebook Thực hiện phân tích tương tác theo thời gian thực với luồng dữ liệu Kinesis.\nTạo bảng Flink bằng Truy vấn SQL Flink\nSử dụng truy vấn Flink SQL để chuyển đổi và tạo luồng dữ liệu mới trong thời gian thực\nThực hiện phát hiện bất thường bằng Chức năng do người dùng xác định Flink và kích hoạt email thông báo bất thường trong thời gian thực.\nCác script ở đây.\nNotebook cũng có tại đây. Bạn có thể tải về và imported thông qua Apache Zeppelin console.\nSau đó, bạn có thể mở notebook và chạy từng đoạn một. Sơ đồ luồng Chạy Apache Zeppelin Chạy các scripts tạo bảng User Defined Function (UDF) thực hiện Phát hiện bất thường trong thời gian thực bằng thuật toán Random Cut Forest algorithm Bạn có thể xem dữ liệu thời gian thực từ các lượt truy cập trang web bằng cách chạy truy vấn ở Bước #3 Tạo impressionstream bằng cách lọc messages từ tickerstream\nTạo clickstream bằng cách lọc messages từ tickerstream Tính toán Tỷ lệ nhấp (CTR) và điền vào ctrstream. Bạn có thể xem Tỷ lệ nhấp trong thời gian thực bằng cách thực hiện Bước 7. Sử dụng UDF (RRandom Cut Forest) để tạo điểm bất thường. Populate anomalydetectionstream bằng cách thực hiện step 9 Bây giờ hãy kiểm tra điểm bất thường từ thuật toán Random Cut Forest trong thời gian thực: Bạn sẽ bắt đầu nhận được thông báo trong email của mình khi phát hiện thấy sự bất thường: Nếu bạn không nhận được email thông báo bất thường trong lần thử đầu tiên\nMở lại hai phiên đồng thời của KDG UI trong trình duyệt của bạn.\nTrong phiên đầu tiên, gửi tin nhắn hiển thị với tốc độ một tin nhắn mỗi giây đến dòng mã đánh dấu, nội dung tin nhắn là\n{\u0026#34;browseraction\u0026#34;:\u0026#34;Impression\u0026#34;, \u0026#34;trang web\u0026#34;:https://www.mysite.com\u0026#34;} Trong phiên thứ hai, gửi tin nhắn nhấp chuột với tốc độ năm tin nhắn mỗi giây đến dòng mã đánh dấu, nội dung tin nhắn là {\u0026#34;browseraction\u0026#34;\u0026gt;Click\u0026#34;, \u0026#34;trang web\u0026#34;:https://www.mysite.com\u0026#34;} Dừng gửi tin nhắn sau 30-40 giây.\nBây giờ trên Sổ ghi chép Apache Zeppelin, hãy lặp lại các bước từ 3 đến 10 và bạn sẽ bắt đầu nhận được thông báo qua email từ lần thử thứ hai.\nDọn dẹp tài nguyên Xong khi hoàn thành bài lab. Delete kda-flink-pre-lab "
},
{
	"uri": "//localhost:1313/2-lab1/",
	"title": "Lab. Phát hiện luồng click chuột bất thường bằng Amazon Managed Service for Apache Flink",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Dữ liệu truyền trực tuyến là dữ liệu được tạo ra liên tục bởi hàng nghìn nguồn dữ liệu, thường gửi các bản ghi dữ liệu đồng thời và ở kích thước nhỏ (Kilobyte). Dữ liệu truyền trực tuyến bao gồm nhiều loại dữ liệu như tệp nhật ký do khách hàng tạo bằng ứng dụng web hoặc thiết bị di động của bạn, mua hàng thương mại điện tử, hoạt động của người chơi trong trò chơi, thông tin từ mạng xã hội, sàn giao dịch tài chính hoặc dịch vụ không gian địa lý và đo từ xa từ các thiết bị được kết nối hoặc thiết bị trong trung tâm dữ liệu.\nDữ liệu này cần được xử lý tuần tự và tăng dần trên cơ sở từng bản ghi hoặc trên các cửa sổ thời gian trượt và được sử dụng cho nhiều loại phân tích bao gồm tương quan, tổng hợp, lọc và lấy mẫu. Thông tin thu được từ phân tích như vậy mang lại cho các công ty cái nhìn sâu sắc về nhiều khía cạnh trong hoạt động kinh doanh và khách hàng của họ, chẳng hạn như –việc sử dụng dịch vụ (để đo lường/thanh toán), hoạt động của máy chủ, số lần nhấp vào trang web và vị trí địa lý của thiết bị, con người và hàng hóa vật lý –và cho phép để họ có thể ứng phó kịp thời với những tình huống mới nổi. Ví dụ: doanh nghiệp có thể theo dõi những thay đổi trong cảm nhận của công chúng đối với thương hiệu và sản phẩm của họ bằng cách liên tục phân tích các luồng truyền thông xã hội và phản hồi kịp thời khi cần thiết.\nAWS có rất nhiều công nghệ mạnh mẽ như Kinesis Data Streams, Kinesis Data Firehose, Amazon Managed Service cho Apache Flink và Managed Streaming cho Kafka khi làm việc với dữ liệu truyền trực tuyến. Trong phòng thí nghiệm này, chúng tôi sẽ đề cập đến một số dịch vụ chính này bằng các bài tập thực hành.\n"
},
{
	"uri": "//localhost:1313/3-ingestionwithdms/3.3-skip-dms-lab/",
	"title": "Option 2: AutoComplete DMS Lab",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Lab trong Data Engineering workshop được thiết kế theo tuần tự. Bài lab này tự động deploy AWS Database Migration Service (AWS DMS) để chúng ta có thể nhanh chóng đến Glue Lab. Nếu muốn hands-on với AWS DMS service. Hãy chọn Option 1: DMS Main Lab\nAutoComplete DMS Chọn \u0026ldquo;Deploy to AWS\u0026rdquo; để deploy stack Deploy To AWS\rStack sẽ hoàn thành các task sau:\nKhởi tạo môi trường cho workshop Tạo DMS subnet group trong VPC Tạo DMS replication instance Tạo source endpoint cho RDS source database Tạo target endpoint cho full data load Tạo target endpoint cho CDC Tạo task thực hiện full migration Tạo task hỗ trợ replication thay đổi. Chọn Parameters: DMSCWRoleCreated: Nếu có role dms-cloudwatch-logs-role thì chọn true, nếu không thì chọn false DMSVPCRoleCreated: Nếu có dms-vpc-role thì chọn true, còn không thì chọn false ServerName: RDS Database Server Name Phía dưới Capabilities, tích checkbox acknowledge the policy và chọn Create stack để tạo.\nStack cần 5-6 phút để hoàn thành. Đợi đến khi \u0026ldquo;CREATE_COMPLETE\u0026rdquo;\nTại thời điểm này, dữ liệu nguồn đã được tải đầy đủ từ cơ sở dữ liệu RDS sang S3 bucket thông qua DMS. Truy cập AWS DMS console , bạn sẽ thấy hai Database migration tasks đã hoàn thành 100%. Nếu không, vui lòng đợi cho đến khi hoàn thành rồi chuyển sang Glue lab "
},
{
	"uri": "//localhost:1313/2-lab1/2.3-labetl/",
	"title": "Lab. Streaming ETL with Glue ",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Trong bài lab này, ta sẽ tìm hiểu cách nhập, xử lý và sử dụng streaming data bằng các dịch vụ serverless của AWS như Kinesis Data Streams, Glue, S3 và Athena. Để mô phỏng đầu vào truyền dữ liệu, chúng tôi sẽ sử dụng Kinesis Data Generator (KDG).\nSetup môi trường Sử dụng DMS Lab Student PreLab CloudFormation để thiết lập môi trường cơ sở hạ tầng hội thảo cốt lõi của bạn. Bỏ qua PreLab tương tự trong phần DMS. Nhấp vào biểu tượng Triển khai lên AWS bên dưới:\nDeploy To AWS\rSet up kinesis stream Mở AWS Kinesis console Chọn “Create data stream” Nhập số liệu như sau: Data stream name: TicketTransactionStreamingData Capacity mode: Provisioned Provisioned shards: 2 Chọn Create data stream Create Table for Kinesis Stream Source in Glue Data Catalog Mở tab AWS Glue console\nTạo database có tên là \u0026ldquo;tickettransactiondatabase\u0026rdquo; Tạo tables có tên là \u0026ldquo;TicketTransactionStreamData\u0026rdquo; ở trong database \u0026ldquo;tickettransactiondatabase\u0026rdquo; Chọn Kinesis làm nguồn, chọn Luồng trong my account để chọn luồng dữ liệu Kinesis, chọn khu vực AWS thích hợp nơi bạn đã tạo luồng, chọn tên luồng là TicketTransactionStreamingData từ danh sách thả xuống, chọn JSON làm định dạng dữ liệu đến, vì chúng ta sẽ gửi JSON payloads từ Kinesis Data Generator theo các bước sau. và nhấp vào Tiếp theo. Để trống schema vì chúng ta sẽ bật tính năng schema detection. Để trống partition indices. Chọn Next Review lại tất cả thông tin và nhấn Create\nChọn vào Table để xem các thuộc tính Tạo và trigger Glue Streaming job Tại mục Data Integration and ETL chọn Glue Studio Chọn Visual with a blank canvas và nhấn Create Chọn Amazon Kinesis từ Source drop down Trong bảng bên phải phía dưới “Data source properties - Kinesis Stream”, cấu hình như sau:\nAmazon Kinesis Source: Data Catalog table Database: tickettransactiondatabase Table: tickettransactionstreamdata Đảm bảo rằng Detect schema được chọn Để tất cả còn lại mặc định Chọn Amazon S3 từ target drop down list Chọn Data target - S3 bucket và cấu hình như sau: Format: Parquet Compression Type: None S3 Target Location: Select Browse S3 and select the “mod-xxx-dmslabs3bucket-xxx” bucket Cuối cùng chọn Job details tab và cấu hình theo như sau: Name: TicketTransactionStreamingJob IAM Role: Select the xxx-GlueLabRole-xxx from the drop down list Type: Spark Streaming Nhấn Save button để tạo job\nKhi thấy Successfully created job ta nhấn Run button để start job\nTrigger streaming data từ Kinesis Data Generator Truy cập Kinesis Data Generator url từ tab setup và đăng nhập. Đảm bảo chọn đúng region. Chọn TicketTransactionStreamingData là target Kinesis stream để Records per second mặc định (100 records per second). Đối với template, nhập NormalTransaction, copy và dán template payload như sau: {\r\u0026#34;customerId\u0026#34;: \u0026#34;{{random.number(50)}}\u0026#34;,\r\u0026#34;transactionAmount\u0026#34;: {{random.number(\r{\r\u0026#34;min\u0026#34;:10,\r\u0026#34;max\u0026#34;:150\r}\r)}},\r\u0026#34;sourceIp\u0026#34; : \u0026#34;{{internet.ip}}\u0026#34;,\r\u0026#34;status\u0026#34;: \u0026#34;{{random.weightedArrayElement({\r\u0026#34;weights\u0026#34; : [0.8,0.1,0.1],\r\u0026#34;data\u0026#34;: [\u0026#34;OK\u0026#34;,\u0026#34;FAIL\u0026#34;,\u0026#34;PENDING\u0026#34;]\r} )}}\u0026#34;,\r\u0026#34;transactionTime\u0026#34;: \u0026#34;{{date.now}}\u0026#34; } Click Send data để trigger transaction streaming data. Tạo Glue Crawler để transformed data Truy cập AWS Glue console Tại AWS Glue menu, chọn Crawlers and click Add crawler\nNhập tên crawler là TicketTransactionParquetDataCrawler, nhấn Next Click vào Add a datasource Chọn S3 và chỉ định path Sau khi thêm datasource, nhấn next\nChọn IAM Role và nhấn Next Chọn prefix là parquet_ cho tables Đăt Crawler Schedule chạy mỗi giờ. Review lại Crawler và Click Create để tạo Crawler Sau khi Crawler tạo xong. Nhấn Run crawler để trigger lần đầu.\nKhi crawler job stop, chuyển đến Glue Data catalog. Đảm bảo rằng parquet_tickettransactionstreamingdata table xuất hiện Click vào parquet_tickettransactionstreamingdata table để xem chi tiết Trigger dữ liệu bất thường từ Kinesis Data Generator(KDG) Mở Kinesis Data Generator, chọn đúng region. Chọn TicketTransactionStreamingData là Kinesis stream đích Template cho record\n{\r\u0026#34;customerId\u0026#34;: \u0026#34;{{random.number(50)}}\u0026#34;,\r\u0026#34;transactionAmount\u0026#34;: {{random.number(\r{\r\u0026#34;min\u0026#34;:10,\r\u0026#34;max\u0026#34;:150\r}\r)}},\r\u0026#34;sourceIp\u0026#34; : \u0026#34;221.233.116.256\u0026#34;,\r\u0026#34;status\u0026#34;: \u0026#34;{{random.weightedArrayElement({\r\u0026#34;weights\u0026#34; : [0.8,0.1,0.1],\r\u0026#34;data\u0026#34;: [\u0026#34;OK\u0026#34;,\u0026#34;FAIL\u0026#34;,\u0026#34;PENDING\u0026#34;]\r} )}}\u0026#34;,\r\u0026#34;transactionTime\u0026#34;: \u0026#34;{{date.now}}\u0026#34; } Click send data Sử dụng Athena để truy vấn dữ liệu Mở AWS Athena console Chọn AwsDataCatalog làm data source và tickettransactiondatabase là database Sử dụng các truy vấn sau để xem dữ liệu\nSELECT count(*) as numberOfTransactions, sourceip\rFROM \u0026#34;tickettransactiondatabase\u0026#34;.\u0026#34;parquet_tickettransactionstreamingdata\u0026#34; WHERE ingest_year=\u0026#39;2024\u0026#39;\rAND cast(ingest_year as bigint)=year(now())\rAND cast(ingest_month as bigint)=month(now())\rAND cast(ingest_day as bigint)=day_of_month(now())\rAND cast(ingest_hour as bigint)=hour(now())\rGROUP BY sourceip\rOrder by numberOfTransactions DESC; "
},
{
	"uri": "//localhost:1313/3-ingestionwithdms/",
	"title": "Nhập data với DMS",
	"tags": [],
	"description": "",
	"content": "Giới thiệu AWS Database Migration Service (AWS DMS) giúp bạn di chuyển cơ sở dữ liệu sang AWS một cách nhanh chóng và an toàn. Cơ sở dữ liệu nguồn vẫn hoạt động đầy đủ trong quá trình di chuyển, giảm thiểu thời gian ngừng hoạt động đối với các ứng dụng dựa trên cơ sở dữ liệu. AWS Database Migration Service có thể di chuyển dữ liệu của bạn đến và đi từ các cơ sở dữ liệu thương mại và nguồn mở được sử dụng rộng rãi nhất.\nAWS Database Migration Service hỗ trợ di chuyển đồng nhất như Oracle sang Oracle cũng như di chuyển không đồng nhất giữa các nền tảng cơ sở dữ liệu khác nhau, chẳng hạn như Oracle hoặc Microsoft SQL Server sang Amazon Aurora. Với AWS Database Migration Service, bạn cũng có thể liên tục sao chép dữ liệu với độ trễ thấp từ bất kỳ nguồn được hỗ trợ nào sang bất kỳ mục tiêu được hỗ trợ nào. Ví dụ: bạn có thể sao chép từ nhiều nguồn sang Amazon Simple Storage Service (Amazon S3) để xây dựng giải pháp hồ dữ liệu có tính sẵn sàng cao và có khả năng mở rộng. Bạn cũng có thể hợp nhất cơ sở dữ liệu vào kho dữ liệu quy mô petabyte bằng cách truyền dữ liệu tới Amazon Redshift.\nCác điều hướng của task\nDMS lab có 3 lựa chọn:\nNếu bạn muốn có trải nghiệm thực hành chuyên sâu về DMS (Dịch vụ di chuyển dữ liệu), trước tiên hãy chạy phòng thí nghiệm của người hướng dẫn để mô phỏng môi trường cơ sở dữ liệu quan hệ tại chỗ, sau đó là phòng thí nghiệm dành cho sinh viên để tạo cơ sở hạ tầng di chuyển dữ liệu cần thiết trong AWS . Phòng thí nghiệm chính sẽ giúp bạn thực hiện di chuyển dữ liệu thực tế từ cơ sở dữ liệu quan hệ sang kho dữ liệu trong AWS. Nếu bạn muốn bắt đầu với Glue ETL và bỏ qua phần thực hành DMS hoàn toàn, vui lòng chạy phòng thí nghiệm tự động hoàn thành DMS. Tự động hoàn thành có thể mất từ 15 đến 20 phút để cung cấp tất cả dữ liệu trong lớp thô của hồ dữ liệu S3 tập trung và bạn sẽ sẵn sàng tìm hiểu sâu về chuyển đổi dữ liệu bằng Glue ETL. Nếu bạn không muốn lab dịch vụ DMS, có thể sao chép trực tiếp dữ liệu thô sang S3 bằng Tùy chọn 3: Bỏ qua DMS Lab. Hạn chế của tùy chọn này là bạn không thể sử dụng DMS để tạo dữ liệu gia tăng nhằm kiểm tra tính năng bookmark của Glue. "
},
{
	"uri": "//localhost:1313/3-ingestionwithdms/3.2-private-instance/",
	"title": "Option 3: Skip DMS Lab",
	"tags": [],
	"description": "",
	"content": "Steps Mở AWS CloudShell Copy data từ staging Amazon S3 bucket đến S3 bucket của chúng ta. Kiểm chứng data Kiến trúc. Cơ sở dữ liệu RDS Postgres được sử dụng làm nguồn bán vé cho các sự kiện thể thao. Nó lưu trữ thông tin giao dịch về giá bán vé cho những người được chọn và chuyển quyền sở hữu vé với các bảng bổ sung để biết chi tiết sự kiện. AWS Database Migration Service (DMS) được sử dụng để tải toàn bộ dữ liệu từ nguồn Amazon RDS sang bộ chứa Amazon S3.\nTrước khi Glue lab bắt đầu, ta có thể chọn bỏ qua quá trình DMS data migration. Nếu vậy, hãy sao chép trực tiếp dữ liệu nguồn vào bộ chứa S3 của ta.\nTrong bài lab hôm nay, ta sẽ sao chép dữ liệu từ bộ chứa S3 tập trung vào tài khoản AWS của mình, thu thập dữ liệu bằng trình thu thập thông tin AWS Glue để tạo siêu dữ liệu, chuyển đổi dữ liệu bằng AWS Glue, truy vấn dữ liệu và tạo Chế độ xem bằng Athena và cuối cùng là xây dựng dashboard với Amazon QuickSight.\nOpen AWS CloudShell Mở AWS CloudShell\nCopy data từ staging Amazon S3 bucket đến S3 bucket của chúng ta. aws s3 cp --recursive --copy-props none s3://aws-dataengineering-day.workshop.aws/data/ s3://\u0026lt;YourBucketName\u0026gt;/tickets/ Data sau đây sẽ được copy đến s3 bucket của chúng ta: Xác minh dữ liệu Mở S3 console và xem dữ liệu được copy từ CloudShell Sử dụng S3 select để truy vấn s3 data Next Step. Tiếp theo, Chúng ta sẽ sử hoàn thành bài lab với AWS Glue\nExtract, Transform and Load Data Lake with AWS Glue "
},
{
	"uri": "//localhost:1313/4-transformingdatawithglue/",
	"title": "Lab: Transforming data with Glue",
	"tags": [],
	"description": "",
	"content": "With Session Manager, we can view the history of connections to instances through Session history. However, we have not seen the details of the commands used in a session.\nIn this section, we will proceed to create an S3 bucket and configure the session logs feature to see the details of the commands used in the session.\nContent: Update IAM Role Create S3 Bucket Create S3 Gateway endpoint Configure Session logs "
},
{
	"uri": "//localhost:1313/2-lab1/2.4-kinesis/",
	"title": "Real-time Streaming with Kinesis",
	"tags": [],
	"description": "",
	"content": "Real-time Streaming with Kinesis Nếu bạn muốn có thêm các lai lab về Kinesis, truy cập Real-time Streaming with Kinesis để xem nhiều hơn\n"
},
{
	"uri": "//localhost:1313/2-lab1/2.5-msk/",
	"title": "Clickstream Analytics using MSK",
	"tags": [],
	"description": "",
	"content": "Clickstream Analytics using MSK Trong workshop này, mục tiêu chung của chúng ta là trực quan hóa và phân tích hiệu suất của các sản phẩm khác nhau trong trang web thương mại điện tử bằng cách nhập, chuyển đổi và phân tích dữ liệu luồng nhấp chuột theo thời gian thực bằng cách sử dụng dịch vụ AWS cho Apache Kafka (Amazon MSK), Apache Flink (Dữ liệu Kinesis Analytics cho các ứng dụng Java) và Elaticsearch (Amazon Elaticsearch). Clickstream Analytics using MSK để tìm hiểu thêm.\nKiến trúc cấp cao sẽ trông như thế này:\nTrước tiên, chúng ta sẽ sử dụng trình tạo dữ liệu để tạo thông báo Clickstream tới một topic (ExampleTopic) trong cụm Amazon MSK lưu trữ Apache kafka. Sau đó, chúng ta sẽ cấu hình và khởi động Kinesis Data Analytics cho Ứng dụng Java bằng cách sử dụng công cụ Apache Flink, một dịch vụ Apache Flink được quản lý từ AWS. Công cụ này sẽ đọc các sự kiện từ chủ đề exampleTopic trong Amazon MSK, xử lý và tổng hợp nó, sau đó gửi các sự kiện tổng hợp ( Analytics) cho cả hai topic trong Amazon MSK và Amazon Elasticsearch. Sau đó, chúng tôi sẽ sử dụng các thông báo đó từ Amazon MSK để minh họa cách người tiêu dùng có thể nhận được phân tích luồng nhấp chuột theo thời gian thực. Ngoài ra, chúng tôi sẽ tạo trực quan hóa Kibana và bảng điều khiển Kibana để trực quan hóa phân tích luồng nhấp chuột theo thời gian thực.\nChúng tôi sẽ cung cấp các tài nguyên cần thiết cho bài lab thông qua CloudFormation.\nTemplate sẽ trông như sau: The stack sẽ gồm:\nA VPC với 1 Public subnet và 3 Private subnets 1 Public instance that hosts a schema registry service, a producer and consumer. 1 Amazon Elasticsearch cluster. 1 Amazon KDA for Java application. 1 Amazon MSK cluster. Trong bài lab này, chúng ta sẽ ssh vào EC2 instance\nKafkaClientEC2Instance\nClickstream Analytics using MSK to follow complete lab.\n"
},
{
	"uri": "//localhost:1313/5-portfwd/",
	"title": "Port Forwarding",
	"tags": [],
	"description": "",
	"content": "\rPort Forwarding is a useful way to redirect network traffic from one IP address - Port to another IP address - Port. With Port Forwarding we can access an EC2 instance located in the private subnet from our workstation.\nWe will configure Port Forwarding for the RDP connection between our machine and Private Windows Instance located in the private subnet we created for this exercise.\nCreate IAM user with permission to connect SSM Go to IAM service management console Click Users , then click Add users. At the Add user page. In the User name field, enter Portfwd. Click on Access key - Programmatic access. Click Next: Permissions. Click Attach existing policies directly.\nIn the search box, enter ssm. Click on AmazonSSMFullAccess. Click Next: Tags, click Next: Reviews. Click Create user. Save Access key ID and Secret access key information to perform AWS CLI configuration.\nInstall and Configure AWS CLI and Session Manager Plugin To perform this hands-on, make sure your workstation has AWS CLI and Session Manager Plugin installed -manager-working-with-install-plugin.html)\nMore hands-on tutorials on installing and configuring the AWS CLI can be found here.\nWith Windows, when extracting the Session Manager Plugin installation folder, run the install.bat file with Administrator permission to perform the installation.\nImplement Portforwarding Run the command below in Command Prompt on your machine to configure Port Forwarding. aws ssm start-session --target (your ID windows instance) --document-name AWS-StartPortForwardingSession --parameters portNumber=\u0026#34;3389\u0026#34;,localPortNumber=\u0026#34;9999\u0026#34; --region (your region) Windows Private Instance Instance ID information can be found when you view the EC2 Windows Private Instance server details.\nExample command: C:\\Windows\\system32\u0026gt;aws ssm start-session --target i-06343d7377486760c --document-name AWS-StartPortForwardingSession --parameters portNumber=\u0026#34;3389\u0026#34;,localPortNumber=\u0026#34;9999\u0026#34; --region ap-southeast-1 If your command gives an error like below: SessionManagerPlugin is not found. Please refer to SessionManager Documentation here: http://docs.aws.amazon.com/console/systems-manager/session-manager-plugin-not-found\nProve that you have not successfully installed the Session Manager Plugin. You may need to relaunch Command Prompt after installing Session Manager Plugin.\nConnect to the Private Windows Instance you created using the Remote Desktop tool on your workstation. In the Computer section: enter localhost:9999. Return to the administration interface of the System Manager - Session Manager service. Click tab Session history. We will see session logs with Document name AWS-StartPortForwardingSession. Congratulations on completing the lab on how to use Session Manager to connect and store session logs in S3 bucket. Remember to perform resource cleanup to avoid unintended costs.\n"
},
{
	"uri": "//localhost:1313/5-portfwd/_index.vi/",
	"title": "Port Forwarding",
	"tags": [],
	"description": "",
	"content": "\rPort Forwarding là mốt cách thức hữu ích để chuyển hướng lưu lượng mạng từ 1 địa chỉ IP - Port này sang 1 địa chỉ IP - Port khác. Với Port Forwarding chúng ta có thể truy cập một EC2 instance nằm trong private subnet từ máy trạm của chúng ta.\nChúng ta sẽ cấu hình Port Forwarding cho kết nối RDP giữa máy của mình với Private Windows Instance nằm trong private subnet mà chúng ta đã tạo cho bài thực hành này.\nTạo IAM User có quyền kết nối SSM Truy cập vào giao diện quản trị dịch vụ IAM Click Users , sau đó click Add users. Tại trang Add user. Tại mục User name, điền Portfwd. Click chọn Access key - Programmatic access. Click Next: Permissions. Click Attach existing policies directly. Tại ô tìm kiếm , điền ssm. Click chọn AmazonSSMFullAccess. Click Next: Tags, click Next: Reviews. Click Create user. Lưu lại thông tin Access key ID và Secret access key để thực hiện cấu hình AWS CLI. Cài đặt và cấu hình AWS CLI và Session Manager Plugin Để thực hiện phần thực hành này, đảm bảo máy trạm của bạn đã cài AWS CLI và Session Manager Plugin\nBạn có thể tham khảo thêm bài thực hành về cài đặt và cấu hình AWS CLI tại đây.\nVới Windows thì khi giải nén thư mục cài đặt Session Manager Plugin bạn hãy chạy file install.bat với quyền Administrator để thực hiện cài đặt.\nThực hiện Portforwarding Chạy command dưới đây trong Command Prompt trên máy của bạn để cấu hình Port Forwarding. aws ssm start-session --target (your ID windows instance) --document-name AWS-StartPortForwardingSession --parameters portNumber=\u0026#34;3389\u0026#34;,localPortNumber=\u0026#34;9999\u0026#34; --region (your region) Thông tin Instance ID của Windows Private Instance có thể tìm được khi bạn xem chi tiết máy chủ EC2 Windows Private Instance.\nCâu lệnh ví dụ C:\\Windows\\system32\u0026gt;aws ssm start-session --target i-06343d7377486760c --document-name AWS-StartPortForwardingSession --parameters portNumber=\u0026#34;3389\u0026#34;,localPortNumber=\u0026#34;9999\u0026#34; --region ap-southeast-1 Nếu câu lệnh của bạn báo lỗi như dưới đây : SessionManagerPlugin is not found. Please refer to SessionManager Documentation here: http://docs.aws.amazon.com/console/systems-manager/session-manager-plugin-not-found\nChứng tỏ bạn chưa cài Session Manager Plugin thành công. Bạn có thể cần khởi chạy lại Command Prompt sau khi cài Session Manager Plugin.\nKết nối tới Private Windows Instance bạn đã tạo bằng công cụ Remote Desktop trên máy trạm của bạn. Tại mục Computer: điền localhost:9999. Quay trở lại giao diện quản trị của dịch vụ System Manager - Session Manager. Click tab Session history. Chúng ta sẽ thấy các session logs với tên Document là AWS-StartPortForwardingSession. Chúc mừng bạn đã hoàn tất bài thực hành hướng dẫn cách sử dụng Session Manager để kết nối cũng như lưu trữ các session logs trong S3 bucket. Hãy nhớ thực hiện bước dọn dẹp tài nguyên để tránh sinh chi phí ngoài ý muốn nhé.\n"
},
{
	"uri": "//localhost:1313/6-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "We will take the following steps to delete the resources we created in this exercise.\nDelete EC2 instance Go to EC2 service management console\nClick Instances. Select both Public Linux Instance and Private Windows Instance instances. Click Instance state. Click Terminate instance, then click Terminate to confirm. Go to IAM service management console\nClick Roles. In the search box, enter SSM. Click to select SSM-Role. Click Delete, then enter the role name SSM-Role and click Delete to delete the role. Click Users. Click on user Portfwd. Click Delete, then enter the user name Portfwd and click Delete to delete the user. Delete S3 bucket Access System Manager - Session Manager service management console.\nClick the Preferences tab. Click Edit. Scroll down. In the section S3 logging. Uncheck Enable to disable logging. Scroll down. Click Save. Go to S3 service management console\nClick on the S3 bucket we created for this lab. (Example: lab-fcj-bucket-0001 ) Click Empty. Enter permanently delete, then click Empty to proceed to delete the object in the bucket. Click Exit. After deleting all objects in the bucket, click Delete\nEnter the name of the S3 bucket, then click Delete bucket to proceed with deleting the S3 bucket. Delete VPC Endpoints Go to VPC service management console Click Endpoints. Select the 4 endpoints we created for the lab including SSM, SSMMESSAGES, EC2MESSAGES, S3GW. Click Actions. Click Delete VPC endpoints. In the confirm box, enter delete.\nClick Delete to proceed with deleting endpoints. Click the refresh icon, check that all endpoints have been deleted before proceeding to the next step.\nDelete VPC Go to VPC service management console\nClick Your VPCs. Click on Lab VPC. Click Actions. Click Delete VPC. In the confirm box, enter delete to confirm, click Delete to delete Lab VPC and related resources.\n"
},
{
	"uri": "//localhost:1313/6-cleanup/_index.vi/",
	"title": "Dọn dẹp tài nguyên  ",
	"tags": [],
	"description": "",
	"content": "Chúng ta sẽ tiến hành các bước sau để xóa các tài nguyên chúng ta đã tạo trong bài thực hành này.\nXóa EC2 instance Truy cập giao diện quản trị dịch vụ EC2 Click Instances. Click chọn cả 2 instance Public Linux Instance và Private Windows Instance. Click Instance state. Click Terminate instance, sau đó click Terminate để xác nhận. Truy cập giao diện quản trị dịch vụ IAM Click Roles. Tại ô tìm kiếm , điền SSM. Click chọn SSM-Role. Click Delete, sau đó điền tên role SSM-Role và click Delete để xóa role. Click Users. Click chọn user Portfwd. Click Delete, sau đó điền tên user Portfwd và click Delete để xóa user. Xóa S3 bucket Truy cập giao diện quản trị dịch vụ System Manager - Session Manager. Click tab Preferences. Click Edit. Kéo chuột xuống dưới. Tại mục S3 logging. Bỏ chọn Enable để tắt tính năng logging. Kéo chuột xuống dưới. Click Save. Truy cập giao diện quản trị dịch vụ S3 Click chọn S3 bucket chúng ta đã tạo cho bài thực hành. ( Ví dụ : lab-fcj-bucket-0001 ) Click Empty. Điền permanently delete, sau đó click Empty để tiến hành xóa object trong bucket. Click Exit. Sau khi xóa hết object trong bucket, click Delete Điền tên S3 bucket, sau đó click Delete bucket để tiến hành xóa S3 bucket. Xóa các VPC Endpoint Truy cập vào giao diện quản trị dịch vụ VPC Click Endpoints. Chọn 4 endpoints chúng ta đã tạo cho bài thực hành bao gồm SSM, SSMMESSAGES, EC2MESSAGES, S3GW. Click Actions. Click Delete VPC endpoints. Tại ô confirm , điền delete. Click Delete để tiến hành xóa các endpoints. Click biểu tượng refresh, kiểm tra tất cả các endpoints đã bị xóa trước khi làm bước tiếp theo. Xóa VPC Truy cập vào giao diện quản trị dịch vụ VPC Click Your VPCs. Click chọn Lab VPC. Click Actions. Click Delete VPC. Tại ô confirm, điền delete để xác nhận, click Delete để thực hiện xóa Lab VPC và các tài nguyên liên quan. "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]